{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 5228\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Imports\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import locale\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# classifiers\n",
    "from sklearn.naive_bayes import GaussianNB # naive bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN\n",
    "from sklearn.linear_model import LogisticRegression # logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier # decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "locale.setlocale(locale.LC_ALL,'')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Training Data\n",
    "drop_columns = ['CreateJob','RetainedJob','City','Name','Zip','BankState']\n",
    "\n",
    "# drop_columns = []\n",
    "\n",
    "le = generate_labels()\n",
    "\n",
    "base_dropna = get_data(le=le,type='train', dropna=True, get_dummy=True, feature_split=False, values_only=True,drop_columns=drop_columns)\n",
    "base_fillna = get_data(le=le,type='train', dropna=False, get_dummy=True, feature_split=False, values_only=True,drop_columns=drop_columns)\n",
    "feature_dropna = get_data(le=le,type='train', dropna=True, get_dummy=True, feature_split=True, values_only=True,drop_columns=drop_columns)\n",
    "feature_fillna = get_data(le=le,type='train', dropna=False, get_dummy=True, feature_split=True, values_only=True,drop_columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49808 entries, 0 to 49999\n",
      "Data columns (total 22 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              49808 non-null  int32  \n",
      " 1   Bank               49808 non-null  int32  \n",
      " 2   NAICS              49808 non-null  int32  \n",
      " 3   ApprovalDate       49808 non-null  int64  \n",
      " 4   ApprovalFY         49808 non-null  int16  \n",
      " 5   Term               49808 non-null  int64  \n",
      " 6   NoEmp              49808 non-null  int64  \n",
      " 7   FranchiseCode      49808 non-null  int32  \n",
      " 8   DisbursementDate   49808 non-null  int64  \n",
      " 9   DisbursementGross  49808 non-null  float32\n",
      " 10  GrAppv             49808 non-null  float32\n",
      " 11  SBA_Appv           49808 non-null  float32\n",
      " 12  ChargeOff          49808 non-null  int64  \n",
      " 13  NewExist_1         49808 non-null  uint8  \n",
      " 14  NewExist_2         49808 non-null  uint8  \n",
      " 15  UrbanRural_0       49808 non-null  uint8  \n",
      " 16  UrbanRural_1       49808 non-null  uint8  \n",
      " 17  UrbanRural_2       49808 non-null  uint8  \n",
      " 18  RevLineCr_N        49808 non-null  uint8  \n",
      " 19  RevLineCr_Y        49808 non-null  uint8  \n",
      " 20  LowDoc_N           49808 non-null  uint8  \n",
      " 21  LowDoc_Y           49808 non-null  uint8  \n",
      "dtypes: float32(3), int16(1), int32(4), int64(5), uint8(9)\n",
      "memory usage: 4.1 MB\n"
     ]
    }
   ],
   "source": [
    "base_dropna.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Test Data\n",
    "feature_test = get_data(le=le,type='test', dropna=False, get_dummy=True, feature_split=True, values_only=True,drop_columns=drop_columns)\n",
    "base_test = get_data(le=le,type='test', dropna=False, get_dummy=True, feature_split=False, values_only=True,drop_columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Bank</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>ApprovalDate</th>\n",
       "      <th>ApprovalFY</th>\n",
       "      <th>FranchiseCode</th>\n",
       "      <th>DisbursementDate</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>NewExist_1</th>\n",
       "      <th>NewExist_2</th>\n",
       "      <th>UrbanRural_0</th>\n",
       "      <th>UrbanRural_1</th>\n",
       "      <th>UrbanRural_2</th>\n",
       "      <th>RevLineCr_N</th>\n",
       "      <th>RevLineCr_Y</th>\n",
       "      <th>LowDoc_N</th>\n",
       "      <th>LowDoc_Y</th>\n",
       "      <th>NoEmp_Micro</th>\n",
       "      <th>NoEmp_Small</th>\n",
       "      <th>NoEmp_Medium</th>\n",
       "      <th>NoEmp_Large</th>\n",
       "      <th>Term_Short</th>\n",
       "      <th>Term_Intermediate</th>\n",
       "      <th>Term_Long</th>\n",
       "      <th>Term_Extra Long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.311220</td>\n",
       "      <td>1872.726350</td>\n",
       "      <td>421790.956760</td>\n",
       "      <td>1.023636e+09</td>\n",
       "      <td>2002.426590</td>\n",
       "      <td>2595.417730</td>\n",
       "      <td>1.033001e+09</td>\n",
       "      <td>1.750579e+05</td>\n",
       "      <td>1.647051e+05</td>\n",
       "      <td>1.244969e+05</td>\n",
       "      <td>0.708940</td>\n",
       "      <td>0.289900</td>\n",
       "      <td>0.275280</td>\n",
       "      <td>0.60408</td>\n",
       "      <td>0.12064</td>\n",
       "      <td>0.737600</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.89246</td>\n",
       "      <td>0.099440</td>\n",
       "      <td>0.762720</td>\n",
       "      <td>0.210320</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.052240</td>\n",
       "      <td>0.12843</td>\n",
       "      <td>0.786920</td>\n",
       "      <td>0.032410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.134917</td>\n",
       "      <td>1325.081943</td>\n",
       "      <td>250716.950871</td>\n",
       "      <td>1.877301e+08</td>\n",
       "      <td>6.208961</td>\n",
       "      <td>12362.965686</td>\n",
       "      <td>1.865782e+08</td>\n",
       "      <td>2.691642e+05</td>\n",
       "      <td>2.652480e+05</td>\n",
       "      <td>2.096262e+05</td>\n",
       "      <td>0.454253</td>\n",
       "      <td>0.453718</td>\n",
       "      <td>0.446657</td>\n",
       "      <td>0.48905</td>\n",
       "      <td>0.32571</td>\n",
       "      <td>0.439941</td>\n",
       "      <td>0.439941</td>\n",
       "      <td>0.30980</td>\n",
       "      <td>0.299253</td>\n",
       "      <td>0.425417</td>\n",
       "      <td>0.407538</td>\n",
       "      <td>0.158688</td>\n",
       "      <td>0.033298</td>\n",
       "      <td>0.222512</td>\n",
       "      <td>0.33457</td>\n",
       "      <td>0.409486</td>\n",
       "      <td>0.177087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.978880e+07</td>\n",
       "      <td>1968.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.641920e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+02</td>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>238220.000000</td>\n",
       "      <td>9.125568e+08</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.254304e+08</td>\n",
       "      <td>3.500000e+04</td>\n",
       "      <td>2.500000e+04</td>\n",
       "      <td>1.600000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>1892.500000</td>\n",
       "      <td>448210.000000</td>\n",
       "      <td>1.085443e+09</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.093910e+09</td>\n",
       "      <td>8.100000e+04</td>\n",
       "      <td>6.500000e+04</td>\n",
       "      <td>4.300000e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>3017.000000</td>\n",
       "      <td>561790.000000</td>\n",
       "      <td>1.159942e+09</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.164845e+09</td>\n",
       "      <td>1.940000e+05</td>\n",
       "      <td>1.750000e+05</td>\n",
       "      <td>1.330000e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>4015.000000</td>\n",
       "      <td>928120.000000</td>\n",
       "      <td>1.399853e+09</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>91350.000000</td>\n",
       "      <td>1.483229e+09</td>\n",
       "      <td>8.995000e+06</td>\n",
       "      <td>5.000000e+06</td>\n",
       "      <td>4.500000e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               State           Bank          NAICS  ApprovalDate  \\\n",
       "count  100000.000000  100000.000000  100000.000000  1.000000e+05   \n",
       "mean       24.311220    1872.726350  421790.956760  1.023636e+09   \n",
       "std        15.134917    1325.081943  250716.950871  1.877301e+08   \n",
       "min         0.000000       0.000000       0.000000 -5.978880e+07   \n",
       "25%        10.000000     574.000000  238220.000000  9.125568e+08   \n",
       "50%        24.000000    1892.500000  448210.000000  1.085443e+09   \n",
       "75%        37.000000    3017.000000  561790.000000  1.159942e+09   \n",
       "max        51.000000    4015.000000  928120.000000  1.399853e+09   \n",
       "\n",
       "          ApprovalFY  FranchiseCode  DisbursementDate  DisbursementGross  \\\n",
       "count  100000.000000  100000.000000      1.000000e+05       1.000000e+05   \n",
       "mean     2002.426590    2595.417730      1.033001e+09       1.750579e+05   \n",
       "std         6.208961   12362.965686      1.865782e+08       2.691642e+05   \n",
       "min      1968.000000       0.000000     -5.641920e+07       0.000000e+00   \n",
       "25%      1999.000000       0.000000      9.254304e+08       3.500000e+04   \n",
       "50%      2004.000000       0.000000      1.093910e+09       8.100000e+04   \n",
       "75%      2007.000000       0.000000      1.164845e+09       1.940000e+05   \n",
       "max      2017.000000   91350.000000      1.483229e+09       8.995000e+06   \n",
       "\n",
       "             GrAppv      SBA_Appv     NewExist_1     NewExist_2  \\\n",
       "count  1.000000e+05  1.000000e+05  100000.000000  100000.000000   \n",
       "mean   1.647051e+05  1.244969e+05       0.708940       0.289900   \n",
       "std    2.652480e+05  2.096262e+05       0.454253       0.453718   \n",
       "min    4.000000e+02  2.000000e+02       0.000000       0.000000   \n",
       "25%    2.500000e+04  1.600000e+04       0.000000       0.000000   \n",
       "50%    6.500000e+04  4.300000e+04       1.000000       0.000000   \n",
       "75%    1.750000e+05  1.330000e+05       1.000000       1.000000   \n",
       "max    5.000000e+06  4.500000e+06       1.000000       1.000000   \n",
       "\n",
       "        UrbanRural_0  UrbanRural_1  UrbanRural_2    RevLineCr_N  \\\n",
       "count  100000.000000  100000.00000  100000.00000  100000.000000   \n",
       "mean        0.275280       0.60408       0.12064       0.737600   \n",
       "std         0.446657       0.48905       0.32571       0.439941   \n",
       "min         0.000000       0.00000       0.00000       0.000000   \n",
       "25%         0.000000       0.00000       0.00000       0.000000   \n",
       "50%         0.000000       1.00000       0.00000       1.000000   \n",
       "75%         1.000000       1.00000       0.00000       1.000000   \n",
       "max         1.000000       1.00000       1.00000       1.000000   \n",
       "\n",
       "         RevLineCr_Y      LowDoc_N       LowDoc_Y    NoEmp_Micro  \\\n",
       "count  100000.000000  100000.00000  100000.000000  100000.000000   \n",
       "mean        0.262400       0.89246       0.099440       0.762720   \n",
       "std         0.439941       0.30980       0.299253       0.425417   \n",
       "min         0.000000       0.00000       0.000000       0.000000   \n",
       "25%         0.000000       1.00000       0.000000       1.000000   \n",
       "50%         0.000000       1.00000       0.000000       1.000000   \n",
       "75%         1.000000       1.00000       0.000000       1.000000   \n",
       "max         1.000000       1.00000       1.000000       1.000000   \n",
       "\n",
       "         NoEmp_Small   NoEmp_Medium    NoEmp_Large     Term_Short  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.210320       0.025850       0.001110       0.052240   \n",
       "std         0.407538       0.158688       0.033298       0.222512   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       Term_Intermediate      Term_Long  Term_Extra Long  \n",
       "count       100000.00000  100000.000000    100000.000000  \n",
       "mean             0.12843       0.786920         0.032410  \n",
       "std              0.33457       0.409486         0.177087  \n",
       "min              0.00000       0.000000         0.000000  \n",
       "25%              0.00000       1.000000         0.000000  \n",
       "50%              0.00000       1.000000         0.000000  \n",
       "75%              0.00000       1.000000         0.000000  \n",
       "max              1.00000       1.000000         1.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Bank</th>\n",
       "      <th>BankState</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>ApprovalDate</th>\n",
       "      <th>ApprovalFY</th>\n",
       "      <th>CreateJob</th>\n",
       "      <th>RetainedJob</th>\n",
       "      <th>FranchiseCode</th>\n",
       "      <th>DisbursementDate</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>NewExist_1</th>\n",
       "      <th>NewExist_2</th>\n",
       "      <th>UrbanRural_0</th>\n",
       "      <th>UrbanRural_1</th>\n",
       "      <th>UrbanRural_2</th>\n",
       "      <th>RevLineCr_N</th>\n",
       "      <th>RevLineCr_Y</th>\n",
       "      <th>LowDoc_N</th>\n",
       "      <th>LowDoc_Y</th>\n",
       "      <th>NoEmp_Micro</th>\n",
       "      <th>NoEmp_Small</th>\n",
       "      <th>NoEmp_Medium</th>\n",
       "      <th>NoEmp_Large</th>\n",
       "      <th>Term_Short</th>\n",
       "      <th>Term_Intermediate</th>\n",
       "      <th>Term_Long</th>\n",
       "      <th>Term_Extra Long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19510</td>\n",
       "      <td>1344</td>\n",
       "      <td>35</td>\n",
       "      <td>11209</td>\n",
       "      <td>312</td>\n",
       "      <td>28</td>\n",
       "      <td>445110</td>\n",
       "      <td>1134604800</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1135987200</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119715</td>\n",
       "      <td>5077</td>\n",
       "      <td>4</td>\n",
       "      <td>85297</td>\n",
       "      <td>2137</td>\n",
       "      <td>41</td>\n",
       "      <td>722211</td>\n",
       "      <td>1051747200</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78760</td>\n",
       "      <td>1056931200</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>110500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>6800</td>\n",
       "      <td>44</td>\n",
       "      <td>77450</td>\n",
       "      <td>291</td>\n",
       "      <td>35</td>\n",
       "      <td>423120</td>\n",
       "      <td>1161043200</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>407</td>\n",
       "      <td>1167523200</td>\n",
       "      <td>184000.0</td>\n",
       "      <td>184000.0</td>\n",
       "      <td>138000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34680</td>\n",
       "      <td>6635</td>\n",
       "      <td>18</td>\n",
       "      <td>40337</td>\n",
       "      <td>1033</td>\n",
       "      <td>18</td>\n",
       "      <td>447110</td>\n",
       "      <td>973641600</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>988588800</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>101250.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125633</td>\n",
       "      <td>14103</td>\n",
       "      <td>36</td>\n",
       "      <td>44087</td>\n",
       "      <td>651</td>\n",
       "      <td>47</td>\n",
       "      <td>722110</td>\n",
       "      <td>1130371200</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1135987200</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name   City  State    Zip  Bank  BankState   NAICS  ApprovalDate  \\\n",
       "0   19510   1344     35  11209   312         28  445110    1134604800   \n",
       "1  119715   5077      4  85297  2137         41  722211    1051747200   \n",
       "2     104   6800     44  77450   291         35  423120    1161043200   \n",
       "3   34680   6635     18  40337  1033         18  447110     973641600   \n",
       "4  125633  14103     36  44087   651         47  722110    1130371200   \n",
       "\n",
       "   ApprovalFY  CreateJob  RetainedJob  FranchiseCode  DisbursementDate  \\\n",
       "0      2006.0          0            6              0        1135987200   \n",
       "1      2003.0          0            0          78760        1056931200   \n",
       "2      2007.0          1            1            407        1167523200   \n",
       "3      2001.0          0            0              0         988588800   \n",
       "4      2006.0          1            9              0        1135987200   \n",
       "\n",
       "   DisbursementGross    GrAppv  SBA_Appv  NewExist_1  NewExist_2  \\\n",
       "0           120000.0  100000.0   50000.0           1           0   \n",
       "1           130000.0  130000.0  110500.0           0           1   \n",
       "2           184000.0  184000.0  138000.0           0           1   \n",
       "3            80000.0  135000.0  101250.0           1           0   \n",
       "4            50000.0   50000.0   25000.0           1           0   \n",
       "\n",
       "   UrbanRural_0  UrbanRural_1  UrbanRural_2  RevLineCr_N  RevLineCr_Y  \\\n",
       "0             0             1             0            0            1   \n",
       "1             0             1             0            1            0   \n",
       "2             0             1             0            1            0   \n",
       "3             0             1             0            1            0   \n",
       "4             0             1             0            1            0   \n",
       "\n",
       "   LowDoc_N  LowDoc_Y  NoEmp_Micro  NoEmp_Small  NoEmp_Medium  NoEmp_Large  \\\n",
       "0         1         0            1            0             0            0   \n",
       "1         0         1            0            1             0            0   \n",
       "2         1         0            1            0             0            0   \n",
       "3         1         0            1            0             0            0   \n",
       "4         1         0            1            0             0            0   \n",
       "\n",
       "   Term_Short  Term_Intermediate  Term_Long  Term_Extra Long  \n",
       "0           0                  0          1                0  \n",
       "1           0                  0          1                0  \n",
       "2           0                  0          1                0  \n",
       "3           0                  0          1                0  \n",
       "4           1                  0          0                0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['KNN', 'LR', 'DT', 'RF', 'GBM']\n",
    "base_dropna_f1 = []\n",
    "base_dropna_acc = []\n",
    "base_fillna_f1 = []\n",
    "base_fillna_acc = []\n",
    "feature_dropna_f1 = []\n",
    "feature_dropna_acc = []\n",
    "feature_fillna_f1 = []\n",
    "feature_fillna_acc = []\n",
    "\n",
    "def calculate_acc_and_f1(classifier, x_train, y_train, x_test, y_test):\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    f1 = round(f1_score(y_test, y_pred, average='weighted') * 100, 2)\n",
    "    acc = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
    "    return f1, acc\n",
    "\n",
    "    \n",
    "def train_single_classifier(classifier, df_in, f1_list, acc_list):\n",
    "    df_x = df_in.drop(columns='ChargeOff')\n",
    "    df_y = df_in['ChargeOff']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size = 0.25, random_state=0)\n",
    "    f1, acc = calculate_acc_and_f1(classifier, x_train, y_train, x_test, y_test)\n",
    "    f1_list.append(f1)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "\n",
    "def train_model(df_in, f1_list, acc_list):\n",
    "    train_single_classifier(KNeighborsClassifier(), df_in, f1_list, acc_list)\n",
    "    train_single_classifier(LogisticRegression(), df_in, f1_list, acc_list)\n",
    "    train_single_classifier(DecisionTreeClassifier(), df_in, f1_list, acc_list)\n",
    "    train_single_classifier(RandomForestClassifier(), df_in, f1_list, acc_list)\n",
    "    train_single_classifier(GradientBoostingClassifier(), df_in, f1_list, acc_list)\n",
    "    \n",
    "\n",
    "train_model(base_dropna, base_dropna_f1, base_dropna_acc)\n",
    "train_model(base_fillna, base_fillna_f1, base_fillna_acc)\n",
    "train_model(feature_dropna, feature_dropna_f1, feature_dropna_acc)\n",
    "train_model(feature_fillna, feature_fillna_f1, feature_fillna_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       base_dropna_acc  base_fillna_acc  feature_dropna_acc  \\\n",
      "Model                                                         \n",
      "KNN              68.50           68.460              68.500   \n",
      "LR               63.76           61.220              63.750   \n",
      "DT               87.78           87.260              71.790   \n",
      "RF               90.21           89.850              79.510   \n",
      "GBM              90.10           89.590              77.680   \n",
      "avg              80.07           79.276              72.246   \n",
      "\n",
      "       feature_fillna_acc  acc_mean  \n",
      "Model                                \n",
      "KNN                68.460    68.480  \n",
      "LR                 61.220    62.490  \n",
      "DT                 71.830    79.670  \n",
      "RF                 78.950    84.630  \n",
      "GBM                77.870    83.810  \n",
      "avg                71.666    75.816  \n",
      "\n",
      "\n",
      "       base_dropna_f1  base_fillna_f1  feature_dropna_f1  feature_fillna_f1  \\\n",
      "Model                                                                         \n",
      "KNN            68.490          68.460             68.490             68.460   \n",
      "LR             63.030          59.950             63.010             59.950   \n",
      "DT             87.780          87.250             71.790             71.830   \n",
      "RF             90.210          89.840             79.490             78.910   \n",
      "GBM            90.100          89.590             77.680             77.860   \n",
      "avg            79.922          79.018             72.092             71.402   \n",
      "\n",
      "       F1_mean  \n",
      "Model           \n",
      "KNN     68.470  \n",
      "LR      61.480  \n",
      "DT      79.660  \n",
      "RF      84.610  \n",
      "GBM     83.810  \n",
      "avg     75.606  \n"
     ]
    }
   ],
   "source": [
    "accuracy_record = pd.DataFrame({'Model': model_names, 'base_dropna_acc': base_dropna_acc, 'base_fillna_acc': base_fillna_acc, 'feature_dropna_acc': feature_dropna_acc, 'feature_fillna_acc': feature_fillna_acc})\n",
    "# accuracy_record = pd.DataFrame({'Model': model_names, 'base_dropna_acc': base_dropna_acc, 'feature_dropna_acc': feature_dropna_acc})\n",
    "accuracy_record['acc_mean'] = accuracy_record.mean(axis=1).round(2)\n",
    "accuracy_record.set_index('Model', inplace=True)\n",
    "accuracy_record.loc['avg'] = accuracy_record.mean()\n",
    "\n",
    "F1_record = pd.DataFrame({'Model': model_names, 'base_dropna_f1': base_dropna_f1, 'base_fillna_f1': base_fillna_f1, 'feature_dropna_f1': feature_dropna_f1, 'feature_fillna_f1': feature_fillna_f1})\n",
    "# F1_record = pd.DataFrame({'Model': model_names, 'base_dropna_f1': base_dropna_f1, 'feature_dropna_f1': feature_dropna_f1})\n",
    "F1_record['F1_mean'] = F1_record.mean(axis=1).round(2)\n",
    "F1_record.set_index('Model', inplace=True)\n",
    "F1_record.loc['avg'] = F1_record.mean()\n",
    "\n",
    "print(accuracy_record)\n",
    "print('\\n')\n",
    "print(F1_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "base_dropna_x = base_dropna.drop(columns='ChargeOff')\n",
    "base_dropna_y = base_dropna['ChargeOff']\n",
    "model.fit(base_dropna_x, base_dropna_y)\n",
    "test_pred = model.predict(base_test)\n",
    "pd.DataFrame(test_pred).to_csv('y_pred.csv',header=['ChargeOff'],index_label=\"Id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_grid: \n",
      " {'loss': ['deviance', 'exponential'], 'learning_rate': [0.05, 0.1, 0.3], 'n_estimators': [50, 100, 150], 'subsample': [0.9, 1], 'criterion': ['friedman_mse', 'mse', 'mae']}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   47.8s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  8.3min\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(base_dropna_x, base_dropna_y, test_size = 0.25, random_state=0)\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "# c = np.append(np.logspace(0, 4, 20),[0.001,.009,0.01,.09,1,5,10,25,100])\n",
    "# param_grid = {'loss': ['deviance', 'exponential'],\n",
    "#               'learning_rate': [0.001,0.05,0.1,0.2,0.5],\n",
    "#               'n_estimators':[50,100,200,500,1000],\n",
    "#               'subsample':[0.9,1],\n",
    "#               'criterion':['friedman_mse', 'mse', 'mae'],\n",
    "#               'min_samples_split':[2,5,10]\n",
    "#              }\n",
    "\n",
    "param_grid = {'loss': ['deviance', 'exponential'],\n",
    "              'learning_rate': [0.05,0.1,0.3],\n",
    "              'n_estimators':[50,100,150],\n",
    "              'subsample':[0.9,1],\n",
    "              'criterion':['friedman_mse', 'mse', 'mae'],\n",
    "#               'min_samples_split':[2,5],\n",
    "#               'max_depth':[3,5,7],\n",
    "#               'max_features':['sqrt','log2', None],\n",
    "              \n",
    "             }\n",
    "print('param_grid: \\n',param_grid)\n",
    "\n",
    "lr_cv = GridSearchCV(clf, param_grid,scoring = 'accuracy',verbose=10,n_jobs=-1)\n",
    "lr_cv.fit(x_train, y_train)\n",
    "\n",
    "#Predict values based on new parameters\n",
    "# y_pred_acc = lr_cv.predict(x_test)\n",
    "\n",
    "print(\"Best Parameters\",lr_cv.best_params_)\n",
    "print(\"Best Accuracy :\",lr_cv.best_score_)\n",
    "\n",
    "y_pred= lr_cv.predict(x_test)\n",
    "print(\"Accuracy: \",round(accuracy_score(y_test, y_pred) * 100, 2))\n",
    "print('Weighted F1 Mesure: ',round(f1_score(y_test, y_pred, average='weighted') * 100, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lr_cv.predict(base_test)\n",
    "pd.DataFrame(test_pred).to_csv('y_pred_grid_search.csv',header=['ChargeOff'],index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Bank</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>ApprovalDate</th>\n",
       "      <th>ApprovalFY</th>\n",
       "      <th>Term</th>\n",
       "      <th>NoEmp</th>\n",
       "      <th>FranchiseCode</th>\n",
       "      <th>DisbursementDate</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>NewExist_1</th>\n",
       "      <th>NewExist_2</th>\n",
       "      <th>UrbanRural_0</th>\n",
       "      <th>UrbanRural_1</th>\n",
       "      <th>UrbanRural_2</th>\n",
       "      <th>RevLineCr_N</th>\n",
       "      <th>RevLineCr_Y</th>\n",
       "      <th>LowDoc_N</th>\n",
       "      <th>LowDoc_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>1</td>\n",
       "      <td>1603</td>\n",
       "      <td>621210</td>\n",
       "      <td>700358400</td>\n",
       "      <td>1992</td>\n",
       "      <td>180</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>720489600</td>\n",
       "      <td>660000.0</td>\n",
       "      <td>660000.0</td>\n",
       "      <td>528000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>5</td>\n",
       "      <td>1036</td>\n",
       "      <td>447110</td>\n",
       "      <td>1007683200</td>\n",
       "      <td>2002</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1009756800</td>\n",
       "      <td>672000.0</td>\n",
       "      <td>672000.0</td>\n",
       "      <td>504000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42924</th>\n",
       "      <td>45</td>\n",
       "      <td>2218</td>\n",
       "      <td>423990</td>\n",
       "      <td>1149552000</td>\n",
       "      <td>2006</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1151625600</td>\n",
       "      <td>24803.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34293</th>\n",
       "      <td>29</td>\n",
       "      <td>3332</td>\n",
       "      <td>812320</td>\n",
       "      <td>691891200</td>\n",
       "      <td>1992</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>696816000</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>24300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47616</th>\n",
       "      <td>5</td>\n",
       "      <td>312</td>\n",
       "      <td>334611</td>\n",
       "      <td>1172102400</td>\n",
       "      <td>2007</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1175299200</td>\n",
       "      <td>39000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>17500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       State  Bank   NAICS  ApprovalDate  ApprovalFY  Term  NoEmp  \\\n",
       "1524       1  1603  621210     700358400        1992   180      5   \n",
       "2282       5  1036  447110    1007683200        2002   300      3   \n",
       "42924     45  2218  423990    1149552000        2006    56      2   \n",
       "34293     29  3332  812320     691891200        1992    60      5   \n",
       "47616      5   312  334611    1172102400        2007    65      3   \n",
       "\n",
       "       FranchiseCode  DisbursementDate  DisbursementGross    GrAppv  SBA_Appv  \\\n",
       "1524               0         720489600           660000.0  660000.0  528000.0   \n",
       "2282               0        1009756800           672000.0  672000.0  504000.0   \n",
       "42924              0        1151625600            24803.0   25000.0   12500.0   \n",
       "34293              0         696816000            27000.0   27000.0   24300.0   \n",
       "47616              0        1175299200            39000.0   35000.0   17500.0   \n",
       "\n",
       "       NewExist_1  NewExist_2  UrbanRural_0  UrbanRural_1  UrbanRural_2  \\\n",
       "1524            1           0             1             0             0   \n",
       "2282            1           0             0             1             0   \n",
       "42924           1           0             0             1             0   \n",
       "34293           1           0             1             0             0   \n",
       "47616           1           0             0             1             0   \n",
       "\n",
       "       RevLineCr_N  RevLineCr_Y  LowDoc_N  LowDoc_Y  \n",
       "1524             1            0         1         0  \n",
       "2282             1            0         1         0  \n",
       "42924            1            0         1         0  \n",
       "34293            1            0         1         0  \n",
       "47616            0            1         1         0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.53505702\n",
      "Iteration 2, loss = 0.46601350\n",
      "Iteration 3, loss = 0.45480825\n",
      "Iteration 4, loss = 0.44054997\n",
      "Iteration 5, loss = 0.43225320\n",
      "Iteration 6, loss = 0.42949949\n",
      "Iteration 7, loss = 0.42392469\n",
      "Iteration 8, loss = 0.42155817\n",
      "Iteration 9, loss = 0.42111155\n",
      "Iteration 10, loss = 0.41623220\n",
      "Iteration 11, loss = 0.41380127\n",
      "Iteration 12, loss = 0.41418075\n",
      "Iteration 13, loss = 0.40878194\n",
      "Iteration 14, loss = 0.41094082\n",
      "Iteration 15, loss = 0.40826696\n",
      "Iteration 16, loss = 0.40716668\n",
      "Iteration 17, loss = 0.40606232\n",
      "Iteration 18, loss = 0.40228043\n",
      "Iteration 19, loss = 0.40357506\n",
      "Iteration 20, loss = 0.39837777\n",
      "Iteration 21, loss = 0.39593674\n",
      "Iteration 22, loss = 0.39567324\n",
      "Iteration 23, loss = 0.39461901\n",
      "Iteration 24, loss = 0.39600672\n",
      "Iteration 25, loss = 0.39008423\n",
      "Iteration 26, loss = 0.39190441\n",
      "Iteration 27, loss = 0.38710242\n",
      "Iteration 28, loss = 0.38897809\n",
      "Iteration 29, loss = 0.38753347\n",
      "Iteration 30, loss = 0.38595089\n",
      "Iteration 31, loss = 0.38941013\n",
      "Iteration 32, loss = 0.38519809\n",
      "Iteration 33, loss = 0.38439379\n",
      "Iteration 34, loss = 0.38579934\n",
      "Iteration 35, loss = 0.38162910\n",
      "Iteration 36, loss = 0.38738758\n",
      "Iteration 37, loss = 0.38421325\n",
      "Iteration 38, loss = 0.38071038\n",
      "Iteration 39, loss = 0.38173849\n",
      "Iteration 40, loss = 0.38012110\n",
      "Iteration 41, loss = 0.38175182\n",
      "Iteration 42, loss = 0.37940095\n",
      "Iteration 43, loss = 0.37978015\n",
      "Iteration 44, loss = 0.37807539\n",
      "Iteration 45, loss = 0.37817682\n",
      "Iteration 46, loss = 0.38041108\n",
      "Iteration 47, loss = 0.37985658\n",
      "Iteration 48, loss = 0.37959013\n",
      "Iteration 49, loss = 0.37810258\n",
      "Iteration 50, loss = 0.37972760\n",
      "Iteration 51, loss = 0.38143968\n",
      "Iteration 52, loss = 0.37541318\n",
      "Iteration 53, loss = 0.37460189\n",
      "Iteration 54, loss = 0.37601272\n",
      "Iteration 55, loss = 0.37787136\n",
      "Iteration 56, loss = 0.37486980\n",
      "Iteration 57, loss = 0.37750376\n",
      "Iteration 58, loss = 0.37474271\n",
      "Iteration 59, loss = 0.37501852\n",
      "Iteration 60, loss = 0.37572112\n",
      "Iteration 61, loss = 0.37413215\n",
      "Iteration 62, loss = 0.37586439\n",
      "Iteration 63, loss = 0.37336761\n",
      "Iteration 64, loss = 0.37288636\n",
      "Iteration 65, loss = 0.37751311\n",
      "Iteration 66, loss = 0.37213437\n",
      "Iteration 67, loss = 0.38302570\n",
      "Iteration 68, loss = 0.37527479\n",
      "Iteration 69, loss = 0.37091249\n",
      "Iteration 70, loss = 0.37177914\n",
      "Iteration 71, loss = 0.37235966\n",
      "Iteration 72, loss = 0.37562622\n",
      "Iteration 73, loss = 0.37281949\n",
      "Iteration 74, loss = 0.37356342\n",
      "Iteration 75, loss = 0.37460489\n",
      "Iteration 76, loss = 0.37080503\n",
      "Iteration 77, loss = 0.37351646\n",
      "Iteration 78, loss = 0.37188105\n",
      "Iteration 79, loss = 0.37364976\n",
      "Iteration 80, loss = 0.37107405\n",
      "Iteration 81, loss = 0.37094093\n",
      "Iteration 82, loss = 0.37045980\n",
      "Iteration 83, loss = 0.37689710\n",
      "Iteration 84, loss = 0.36974510\n",
      "Iteration 85, loss = 0.37153482\n",
      "Iteration 86, loss = 0.36865891\n",
      "Iteration 87, loss = 0.37179852\n",
      "Iteration 88, loss = 0.37049795\n",
      "Iteration 89, loss = 0.36950642\n",
      "Iteration 90, loss = 0.36895345\n",
      "Iteration 91, loss = 0.37327925\n",
      "Iteration 92, loss = 0.36855818\n",
      "Iteration 93, loss = 0.36799805\n",
      "Iteration 94, loss = 0.39568996\n",
      "Iteration 95, loss = 0.37260252\n",
      "Iteration 96, loss = 0.36999665\n",
      "Iteration 97, loss = 0.36948728\n",
      "Iteration 98, loss = 0.37254878\n",
      "Iteration 99, loss = 0.37171450\n",
      "Iteration 100, loss = 0.36925283\n",
      "Iteration 101, loss = 0.37160897\n",
      "Iteration 102, loss = 0.36425764\n",
      "Iteration 103, loss = 0.36912677\n",
      "Iteration 104, loss = 0.36772762\n",
      "Iteration 105, loss = 0.36582871\n",
      "Iteration 106, loss = 0.36554350\n",
      "Iteration 107, loss = 0.36539373\n",
      "Iteration 108, loss = 0.36802507\n",
      "Iteration 109, loss = 0.36831180\n",
      "Iteration 110, loss = 0.36358915\n",
      "Iteration 111, loss = 0.36514345\n",
      "Iteration 112, loss = 0.36581283\n",
      "Iteration 113, loss = 0.36544920\n",
      "Iteration 114, loss = 0.36532967\n",
      "Iteration 115, loss = 0.36676549\n",
      "Iteration 116, loss = 0.36592490\n",
      "Iteration 117, loss = 0.36363467\n",
      "Iteration 118, loss = 0.36887013\n",
      "Iteration 119, loss = 0.36321385\n",
      "Iteration 120, loss = 0.36446919\n",
      "Iteration 121, loss = 0.36234191\n",
      "Iteration 122, loss = 0.36349862\n",
      "Iteration 123, loss = 0.36450289\n",
      "Iteration 124, loss = 0.36613662\n",
      "Iteration 125, loss = 0.36619421\n",
      "Iteration 126, loss = 0.36186088\n",
      "Iteration 127, loss = 0.36758109\n",
      "Iteration 128, loss = 0.36252962\n",
      "Iteration 129, loss = 0.36131985\n",
      "Iteration 130, loss = 0.36311918\n",
      "Iteration 131, loss = 0.36370364\n",
      "Iteration 132, loss = 0.36163841\n",
      "Iteration 133, loss = 0.36106982\n",
      "Iteration 134, loss = 0.36400213\n",
      "Iteration 135, loss = 0.35972160\n",
      "Iteration 136, loss = 0.36360306\n",
      "Iteration 137, loss = 0.35841512\n",
      "Iteration 138, loss = 0.36048839\n",
      "Iteration 139, loss = 0.35992156\n",
      "Iteration 140, loss = 0.36121527\n",
      "Iteration 141, loss = 0.36092126\n",
      "Iteration 142, loss = 0.36148423\n",
      "Iteration 143, loss = 0.36087329\n",
      "Iteration 144, loss = 0.36205950\n",
      "Iteration 145, loss = 0.36056737\n",
      "Iteration 146, loss = 0.35749949\n",
      "Iteration 147, loss = 0.35979386\n",
      "Iteration 148, loss = 0.35876635\n",
      "Iteration 149, loss = 0.36156732\n",
      "Iteration 150, loss = 0.35926950\n",
      "Iteration 151, loss = 0.35977158\n",
      "Iteration 152, loss = 0.35701056\n",
      "Iteration 153, loss = 0.35794661\n",
      "Iteration 154, loss = 0.35532654\n",
      "Iteration 155, loss = 0.35649127\n",
      "Iteration 156, loss = 0.35764506\n",
      "Iteration 157, loss = 0.36072889\n",
      "Iteration 158, loss = 0.35918161\n",
      "Iteration 159, loss = 0.35542176\n",
      "Iteration 160, loss = 0.35746969\n",
      "Iteration 161, loss = 0.36143681\n",
      "Iteration 162, loss = 0.35809066\n",
      "Iteration 163, loss = 0.35808186\n",
      "Iteration 164, loss = 0.35605083\n",
      "Iteration 165, loss = 0.35643216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler_x = MinMaxScaler()\n",
    "base_dropna_x = base_dropna.drop(columns='ChargeOff')\n",
    "base_dropna_y = base_dropna['ChargeOff']\n",
    "base_dropna_scaled_x = min_max_scaler_x.fit_transform(base_dropna_x.to_numpy())\n",
    "base_dropna_normalized_x = pd.DataFrame(base_dropna_scaled_x)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(base_dropna_normalized_x, base_dropna_y, test_size = 0.25, random_state=0)\n",
    "clf = MLPClassifier(max_iter=1000, hidden_layer_sizes=(16,8,4), learning_rate_init=0.01, solver='adam', verbose=True).fit(base_dropna_normalized_x, base_dropna_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler_x = MinMaxScaler()\n",
    "base_dropna_x = base_dropna.drop(columns='ChargeOff')\n",
    "base_dropna_y = base_dropna['ChargeOff']\n",
    "base_dropna_scaled_x = min_max_scaler_x.fit_transform(base_dropna_x.to_numpy())\n",
    "base_dropna_normalized_x = pd.DataFrame(base_dropna_scaled_x)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(base_dropna_normalized_x, base_dropna_y, test_size = 0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 85.28, acc: 85.29\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = clf.predict(x_val)\n",
    "f1 = round(f1_score(y_val, y_val_pred, average='weighted') * 100, 2)\n",
    "acc = round(accuracy_score(y_val, y_val_pred) * 100, 2)\n",
    "print(\"f1: {}, acc: {}\".format(f1, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.to_numpy().reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_val = x_val.to_numpy().reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               76800     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 85,121\n",
      "Trainable params: 85,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(128, input_shape=(x_train.shape[1], x_train.shape[2])),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.003),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37356 samples, validate on 12452 samples\n",
      "Epoch 1/300\n",
      "37356/37356 - 2s - loss: 0.1766 - accuracy: 0.7442 - val_loss: 0.1548 - val_accuracy: 0.7767\n",
      "Epoch 2/300\n",
      "37356/37356 - 1s - loss: 0.1481 - accuracy: 0.7966 - val_loss: 0.1454 - val_accuracy: 0.8016\n",
      "Epoch 3/300\n",
      "37356/37356 - 1s - loss: 0.1425 - accuracy: 0.8063 - val_loss: 0.1369 - val_accuracy: 0.8183\n",
      "Epoch 4/300\n",
      "37356/37356 - 1s - loss: 0.1390 - accuracy: 0.8089 - val_loss: 0.1353 - val_accuracy: 0.8179\n",
      "Epoch 5/300\n",
      "37356/37356 - 1s - loss: 0.1367 - accuracy: 0.8139 - val_loss: 0.1332 - val_accuracy: 0.8244\n",
      "Epoch 6/300\n",
      "37356/37356 - 1s - loss: 0.1338 - accuracy: 0.8170 - val_loss: 0.1327 - val_accuracy: 0.8211\n",
      "Epoch 7/300\n",
      "37356/37356 - 1s - loss: 0.1320 - accuracy: 0.8197 - val_loss: 0.1301 - val_accuracy: 0.8232\n",
      "Epoch 8/300\n",
      "37356/37356 - 1s - loss: 0.1295 - accuracy: 0.8216 - val_loss: 0.1336 - val_accuracy: 0.8144\n",
      "Epoch 9/300\n",
      "37356/37356 - 1s - loss: 0.1288 - accuracy: 0.8235 - val_loss: 0.1270 - val_accuracy: 0.8290\n",
      "Epoch 10/300\n",
      "37356/37356 - 1s - loss: 0.1266 - accuracy: 0.8276 - val_loss: 0.1275 - val_accuracy: 0.8265\n",
      "Epoch 11/300\n",
      "37356/37356 - 1s - loss: 0.1233 - accuracy: 0.8323 - val_loss: 0.1246 - val_accuracy: 0.8288\n",
      "Epoch 12/300\n",
      "37356/37356 - 1s - loss: 0.1216 - accuracy: 0.8345 - val_loss: 0.1250 - val_accuracy: 0.8318\n",
      "Epoch 13/300\n",
      "37356/37356 - 1s - loss: 0.1205 - accuracy: 0.8359 - val_loss: 0.1248 - val_accuracy: 0.8355\n",
      "Epoch 14/300\n",
      "37356/37356 - 1s - loss: 0.1187 - accuracy: 0.8410 - val_loss: 0.1193 - val_accuracy: 0.8374\n",
      "Epoch 15/300\n",
      "37356/37356 - 1s - loss: 0.1188 - accuracy: 0.8391 - val_loss: 0.1176 - val_accuracy: 0.8436\n",
      "Epoch 16/300\n",
      "37356/37356 - 1s - loss: 0.1179 - accuracy: 0.8395 - val_loss: 0.1172 - val_accuracy: 0.8418\n",
      "Epoch 17/300\n",
      "37356/37356 - 1s - loss: 0.1165 - accuracy: 0.8419 - val_loss: 0.1179 - val_accuracy: 0.8423\n",
      "Epoch 18/300\n",
      "37356/37356 - 1s - loss: 0.1160 - accuracy: 0.8432 - val_loss: 0.1184 - val_accuracy: 0.8391\n",
      "Epoch 19/300\n",
      "37356/37356 - 1s - loss: 0.1163 - accuracy: 0.8426 - val_loss: 0.1185 - val_accuracy: 0.8399\n",
      "Epoch 20/300\n",
      "37356/37356 - 1s - loss: 0.1152 - accuracy: 0.8446 - val_loss: 0.1163 - val_accuracy: 0.8437\n",
      "Epoch 21/300\n",
      "37356/37356 - 1s - loss: 0.1139 - accuracy: 0.8464 - val_loss: 0.1170 - val_accuracy: 0.8436\n",
      "Epoch 22/300\n",
      "37356/37356 - 1s - loss: 0.1126 - accuracy: 0.8483 - val_loss: 0.1157 - val_accuracy: 0.8461\n",
      "Epoch 23/300\n",
      "37356/37356 - 1s - loss: 0.1133 - accuracy: 0.8470 - val_loss: 0.1154 - val_accuracy: 0.8450\n",
      "Epoch 24/300\n",
      "37356/37356 - 1s - loss: 0.1118 - accuracy: 0.8503 - val_loss: 0.1168 - val_accuracy: 0.8428\n",
      "Epoch 25/300\n",
      "37356/37356 - 1s - loss: 0.1111 - accuracy: 0.8502 - val_loss: 0.1147 - val_accuracy: 0.8448\n",
      "Epoch 26/300\n",
      "37356/37356 - 1s - loss: 0.1115 - accuracy: 0.8496 - val_loss: 0.1155 - val_accuracy: 0.8432\n",
      "Epoch 27/300\n",
      "37356/37356 - 1s - loss: 0.1104 - accuracy: 0.8520 - val_loss: 0.1162 - val_accuracy: 0.8432\n",
      "Epoch 28/300\n",
      "37356/37356 - 1s - loss: 0.1106 - accuracy: 0.8521 - val_loss: 0.1273 - val_accuracy: 0.8282\n",
      "Epoch 29/300\n",
      "37356/37356 - 1s - loss: 0.1094 - accuracy: 0.8537 - val_loss: 0.1137 - val_accuracy: 0.8482\n",
      "Epoch 30/300\n",
      "37356/37356 - 1s - loss: 0.1086 - accuracy: 0.8548 - val_loss: 0.1146 - val_accuracy: 0.8505\n",
      "Epoch 31/300\n",
      "37356/37356 - 1s - loss: 0.1090 - accuracy: 0.8538 - val_loss: 0.1156 - val_accuracy: 0.8428\n",
      "Epoch 32/300\n",
      "37356/37356 - 1s - loss: 0.1084 - accuracy: 0.8543 - val_loss: 0.1125 - val_accuracy: 0.8493\n",
      "Epoch 33/300\n",
      "37356/37356 - 1s - loss: 0.1086 - accuracy: 0.8533 - val_loss: 0.1106 - val_accuracy: 0.8507\n",
      "Epoch 34/300\n",
      "37356/37356 - 1s - loss: 0.1081 - accuracy: 0.8537 - val_loss: 0.1130 - val_accuracy: 0.8484\n",
      "Epoch 35/300\n",
      "37356/37356 - 1s - loss: 0.1067 - accuracy: 0.8576 - val_loss: 0.1130 - val_accuracy: 0.8496\n",
      "Epoch 36/300\n",
      "37356/37356 - 1s - loss: 0.1071 - accuracy: 0.8559 - val_loss: 0.1114 - val_accuracy: 0.8503\n",
      "Epoch 37/300\n",
      "37356/37356 - 1s - loss: 0.1058 - accuracy: 0.8589 - val_loss: 0.1093 - val_accuracy: 0.8538\n",
      "Epoch 38/300\n",
      "37356/37356 - 1s - loss: 0.1058 - accuracy: 0.8573 - val_loss: 0.1161 - val_accuracy: 0.8435\n",
      "Epoch 39/300\n",
      "37356/37356 - 1s - loss: 0.1068 - accuracy: 0.8570 - val_loss: 0.1135 - val_accuracy: 0.8468\n",
      "Epoch 40/300\n",
      "37356/37356 - 1s - loss: 0.1062 - accuracy: 0.8581 - val_loss: 0.1106 - val_accuracy: 0.8509\n",
      "Epoch 41/300\n",
      "37356/37356 - 1s - loss: 0.1052 - accuracy: 0.8595 - val_loss: 0.1142 - val_accuracy: 0.8514\n",
      "Epoch 42/300\n",
      "37356/37356 - 1s - loss: 0.1051 - accuracy: 0.8589 - val_loss: 0.1132 - val_accuracy: 0.8464\n",
      "Epoch 43/300\n",
      "37356/37356 - 1s - loss: 0.1046 - accuracy: 0.8603 - val_loss: 0.1127 - val_accuracy: 0.8503\n",
      "Epoch 44/300\n",
      "37356/37356 - 1s - loss: 0.1045 - accuracy: 0.8605 - val_loss: 0.1109 - val_accuracy: 0.8520\n",
      "Epoch 45/300\n",
      "37356/37356 - 1s - loss: 0.1038 - accuracy: 0.8621 - val_loss: 0.1085 - val_accuracy: 0.8550\n",
      "Epoch 46/300\n",
      "37356/37356 - 1s - loss: 0.1034 - accuracy: 0.8636 - val_loss: 0.1128 - val_accuracy: 0.8478\n",
      "Epoch 47/300\n",
      "37356/37356 - 1s - loss: 0.1046 - accuracy: 0.8615 - val_loss: 0.1129 - val_accuracy: 0.8474\n",
      "Epoch 48/300\n",
      "37356/37356 - 1s - loss: 0.1026 - accuracy: 0.8643 - val_loss: 0.1072 - val_accuracy: 0.8582\n",
      "Epoch 49/300\n",
      "37356/37356 - 1s - loss: 0.1029 - accuracy: 0.8625 - val_loss: 0.1096 - val_accuracy: 0.8525\n",
      "Epoch 50/300\n",
      "37356/37356 - 1s - loss: 0.1032 - accuracy: 0.8628 - val_loss: 0.1099 - val_accuracy: 0.8530\n",
      "Epoch 51/300\n",
      "37356/37356 - 1s - loss: 0.1025 - accuracy: 0.8627 - val_loss: 0.1149 - val_accuracy: 0.8467\n",
      "Epoch 52/300\n",
      "37356/37356 - 1s - loss: 0.1019 - accuracy: 0.8640 - val_loss: 0.1080 - val_accuracy: 0.8569\n",
      "Epoch 53/300\n",
      "37356/37356 - 1s - loss: 0.1017 - accuracy: 0.8643 - val_loss: 0.1091 - val_accuracy: 0.8542\n",
      "Epoch 54/300\n",
      "37356/37356 - 1s - loss: 0.1017 - accuracy: 0.8648 - val_loss: 0.1096 - val_accuracy: 0.8559\n",
      "Epoch 55/300\n",
      "37356/37356 - 1s - loss: 0.1022 - accuracy: 0.8640 - val_loss: 0.1128 - val_accuracy: 0.8452\n",
      "Epoch 56/300\n",
      "37356/37356 - 1s - loss: 0.1012 - accuracy: 0.8644 - val_loss: 0.1183 - val_accuracy: 0.8389\n",
      "Epoch 57/300\n",
      "37356/37356 - 1s - loss: 0.1010 - accuracy: 0.8658 - val_loss: 0.1121 - val_accuracy: 0.8492\n",
      "Epoch 58/300\n",
      "37356/37356 - 1s - loss: 0.1010 - accuracy: 0.8650 - val_loss: 0.1110 - val_accuracy: 0.8506\n",
      "Epoch 59/300\n",
      "37356/37356 - 1s - loss: 0.1006 - accuracy: 0.8657 - val_loss: 0.1078 - val_accuracy: 0.8576\n",
      "Epoch 60/300\n",
      "37356/37356 - 1s - loss: 0.1003 - accuracy: 0.8673 - val_loss: 0.1079 - val_accuracy: 0.8577\n",
      "Epoch 61/300\n",
      "37356/37356 - 1s - loss: 0.0997 - accuracy: 0.8670 - val_loss: 0.1087 - val_accuracy: 0.8536\n",
      "Epoch 62/300\n",
      "37356/37356 - 1s - loss: 0.0996 - accuracy: 0.8678 - val_loss: 0.1102 - val_accuracy: 0.8526\n",
      "Epoch 63/300\n",
      "37356/37356 - 1s - loss: 0.0989 - accuracy: 0.8685 - val_loss: 0.1074 - val_accuracy: 0.8548\n",
      "Epoch 64/300\n",
      "37356/37356 - 1s - loss: 0.0998 - accuracy: 0.8667 - val_loss: 0.1124 - val_accuracy: 0.8522\n",
      "Epoch 65/300\n",
      "37356/37356 - 1s - loss: 0.0992 - accuracy: 0.8685 - val_loss: 0.1088 - val_accuracy: 0.8547\n",
      "Epoch 66/300\n",
      "37356/37356 - 1s - loss: 0.0994 - accuracy: 0.8676 - val_loss: 0.1078 - val_accuracy: 0.8575\n",
      "Epoch 67/300\n",
      "37356/37356 - 1s - loss: 0.0982 - accuracy: 0.8705 - val_loss: 0.1091 - val_accuracy: 0.8528\n",
      "Epoch 68/300\n",
      "37356/37356 - 1s - loss: 0.0993 - accuracy: 0.8681 - val_loss: 0.1084 - val_accuracy: 0.8542\n",
      "Epoch 69/300\n",
      "37356/37356 - 1s - loss: 0.0985 - accuracy: 0.8682 - val_loss: 0.1097 - val_accuracy: 0.8564\n",
      "Epoch 70/300\n",
      "37356/37356 - 1s - loss: 0.0990 - accuracy: 0.8687 - val_loss: 0.1078 - val_accuracy: 0.8550\n",
      "Epoch 71/300\n",
      "37356/37356 - 1s - loss: 0.0985 - accuracy: 0.8696 - val_loss: 0.1089 - val_accuracy: 0.8547\n",
      "Epoch 72/300\n",
      "37356/37356 - 1s - loss: 0.0978 - accuracy: 0.8712 - val_loss: 0.1084 - val_accuracy: 0.8538\n",
      "Epoch 73/300\n",
      "37356/37356 - 1s - loss: 0.0981 - accuracy: 0.8698 - val_loss: 0.1066 - val_accuracy: 0.8587\n",
      "Epoch 74/300\n",
      "37356/37356 - 1s - loss: 0.0979 - accuracy: 0.8706 - val_loss: 0.1088 - val_accuracy: 0.8568\n",
      "Epoch 75/300\n",
      "37356/37356 - 1s - loss: 0.0980 - accuracy: 0.8703 - val_loss: 0.1087 - val_accuracy: 0.8523\n",
      "Epoch 76/300\n",
      "37356/37356 - 1s - loss: 0.0981 - accuracy: 0.8711 - val_loss: 0.1078 - val_accuracy: 0.8579\n",
      "Epoch 77/300\n",
      "37356/37356 - 1s - loss: 0.0970 - accuracy: 0.8719 - val_loss: 0.1071 - val_accuracy: 0.8579\n",
      "Epoch 78/300\n",
      "37356/37356 - 1s - loss: 0.0967 - accuracy: 0.8724 - val_loss: 0.1115 - val_accuracy: 0.8520\n",
      "Epoch 79/300\n",
      "37356/37356 - 1s - loss: 0.0961 - accuracy: 0.8733 - val_loss: 0.1079 - val_accuracy: 0.8555\n",
      "Epoch 80/300\n",
      "37356/37356 - 1s - loss: 0.0965 - accuracy: 0.8706 - val_loss: 0.1074 - val_accuracy: 0.8572\n",
      "Epoch 81/300\n",
      "37356/37356 - 1s - loss: 0.0960 - accuracy: 0.8736 - val_loss: 0.1081 - val_accuracy: 0.8554\n",
      "Epoch 82/300\n",
      "37356/37356 - 1s - loss: 0.0962 - accuracy: 0.8716 - val_loss: 0.1108 - val_accuracy: 0.8508\n",
      "Epoch 83/300\n",
      "37356/37356 - 1s - loss: 0.0963 - accuracy: 0.8729 - val_loss: 0.1080 - val_accuracy: 0.8551\n",
      "Epoch 84/300\n",
      "37356/37356 - 1s - loss: 0.0951 - accuracy: 0.8741 - val_loss: 0.1073 - val_accuracy: 0.8562\n",
      "Epoch 85/300\n",
      "37356/37356 - 1s - loss: 0.0953 - accuracy: 0.8740 - val_loss: 0.1072 - val_accuracy: 0.8547\n",
      "Epoch 86/300\n",
      "37356/37356 - 1s - loss: 0.0962 - accuracy: 0.8730 - val_loss: 0.1085 - val_accuracy: 0.8562\n",
      "Epoch 87/300\n",
      "37356/37356 - 1s - loss: 0.0949 - accuracy: 0.8751 - val_loss: 0.1085 - val_accuracy: 0.8546\n",
      "Epoch 88/300\n",
      "37356/37356 - 1s - loss: 0.0949 - accuracy: 0.8744 - val_loss: 0.1069 - val_accuracy: 0.8561\n",
      "Epoch 89/300\n",
      "37356/37356 - 1s - loss: 0.0949 - accuracy: 0.8743 - val_loss: 0.1078 - val_accuracy: 0.8570\n",
      "Epoch 90/300\n",
      "37356/37356 - 1s - loss: 0.0950 - accuracy: 0.8733 - val_loss: 0.1070 - val_accuracy: 0.8562\n",
      "Epoch 91/300\n",
      "37356/37356 - 1s - loss: 0.0936 - accuracy: 0.8764 - val_loss: 0.1121 - val_accuracy: 0.8511\n",
      "Epoch 92/300\n",
      "37356/37356 - 1s - loss: 0.0939 - accuracy: 0.8749 - val_loss: 0.1069 - val_accuracy: 0.8582\n",
      "Epoch 93/300\n",
      "37356/37356 - 1s - loss: 0.0948 - accuracy: 0.8747 - val_loss: 0.1090 - val_accuracy: 0.8547\n",
      "Epoch 94/300\n",
      "37356/37356 - 1s - loss: 0.0941 - accuracy: 0.8751 - val_loss: 0.1101 - val_accuracy: 0.8513\n",
      "Epoch 95/300\n",
      "37356/37356 - 1s - loss: 0.0935 - accuracy: 0.8764 - val_loss: 0.1063 - val_accuracy: 0.8574\n",
      "Epoch 96/300\n",
      "37356/37356 - 1s - loss: 0.0934 - accuracy: 0.8768 - val_loss: 0.1091 - val_accuracy: 0.8569\n",
      "Epoch 97/300\n",
      "37356/37356 - 1s - loss: 0.0928 - accuracy: 0.8780 - val_loss: 0.1096 - val_accuracy: 0.8532\n",
      "Epoch 98/300\n",
      "37356/37356 - 1s - loss: 0.0928 - accuracy: 0.8770 - val_loss: 0.1072 - val_accuracy: 0.8571\n",
      "Epoch 99/300\n",
      "37356/37356 - 1s - loss: 0.0934 - accuracy: 0.8769 - val_loss: 0.1091 - val_accuracy: 0.8550\n",
      "Epoch 100/300\n",
      "37356/37356 - 1s - loss: 0.0946 - accuracy: 0.8753 - val_loss: 0.1063 - val_accuracy: 0.8594\n",
      "Epoch 101/300\n",
      "37356/37356 - 1s - loss: 0.0926 - accuracy: 0.8769 - val_loss: 0.1089 - val_accuracy: 0.8550\n",
      "Epoch 102/300\n",
      "37356/37356 - 1s - loss: 0.0922 - accuracy: 0.8788 - val_loss: 0.1094 - val_accuracy: 0.8572\n",
      "Epoch 103/300\n",
      "37356/37356 - 1s - loss: 0.0926 - accuracy: 0.8782 - val_loss: 0.1082 - val_accuracy: 0.8579\n",
      "Epoch 104/300\n",
      "37356/37356 - 1s - loss: 0.0921 - accuracy: 0.8784 - val_loss: 0.1068 - val_accuracy: 0.8579\n",
      "Epoch 105/300\n",
      "37356/37356 - 1s - loss: 0.0922 - accuracy: 0.8779 - val_loss: 0.1138 - val_accuracy: 0.8501\n",
      "Epoch 106/300\n",
      "37356/37356 - 1s - loss: 0.0924 - accuracy: 0.8775 - val_loss: 0.1088 - val_accuracy: 0.8536\n",
      "Epoch 107/300\n",
      "37356/37356 - 1s - loss: 0.0918 - accuracy: 0.8791 - val_loss: 0.1088 - val_accuracy: 0.8558\n",
      "Epoch 108/300\n",
      "37356/37356 - 1s - loss: 0.0913 - accuracy: 0.8795 - val_loss: 0.1071 - val_accuracy: 0.8565\n",
      "Epoch 109/300\n",
      "37356/37356 - 1s - loss: 0.0914 - accuracy: 0.8797 - val_loss: 0.1095 - val_accuracy: 0.8542\n",
      "Epoch 110/300\n",
      "37356/37356 - 1s - loss: 0.0915 - accuracy: 0.8792 - val_loss: 0.1104 - val_accuracy: 0.8515\n",
      "Epoch 111/300\n",
      "37356/37356 - 1s - loss: 0.0922 - accuracy: 0.8787 - val_loss: 0.1083 - val_accuracy: 0.8571\n",
      "Epoch 112/300\n",
      "37356/37356 - 1s - loss: 0.0908 - accuracy: 0.8796 - val_loss: 0.1091 - val_accuracy: 0.8562\n",
      "Epoch 113/300\n",
      "37356/37356 - 1s - loss: 0.0899 - accuracy: 0.8810 - val_loss: 0.1092 - val_accuracy: 0.8576\n",
      "Epoch 114/300\n",
      "37356/37356 - 1s - loss: 0.0907 - accuracy: 0.8810 - val_loss: 0.1064 - val_accuracy: 0.8582\n",
      "Epoch 115/300\n",
      "37356/37356 - 1s - loss: 0.0906 - accuracy: 0.8802 - val_loss: 0.1089 - val_accuracy: 0.8558\n",
      "Epoch 116/300\n",
      "37356/37356 - 1s - loss: 0.0895 - accuracy: 0.8826 - val_loss: 0.1153 - val_accuracy: 0.8495\n",
      "Epoch 117/300\n",
      "37356/37356 - 1s - loss: 0.0896 - accuracy: 0.8813 - val_loss: 0.1070 - val_accuracy: 0.8584\n",
      "Epoch 118/300\n",
      "37356/37356 - 1s - loss: 0.0908 - accuracy: 0.8802 - val_loss: 0.1083 - val_accuracy: 0.8570\n",
      "Epoch 119/300\n",
      "37356/37356 - 1s - loss: 0.0903 - accuracy: 0.8808 - val_loss: 0.1065 - val_accuracy: 0.8599\n",
      "Epoch 120/300\n",
      "37356/37356 - 1s - loss: 0.0900 - accuracy: 0.8803 - val_loss: 0.1094 - val_accuracy: 0.8546\n",
      "Epoch 121/300\n",
      "37356/37356 - 1s - loss: 0.0892 - accuracy: 0.8832 - val_loss: 0.1088 - val_accuracy: 0.8575\n",
      "Epoch 122/300\n",
      "37356/37356 - 1s - loss: 0.0890 - accuracy: 0.8843 - val_loss: 0.1095 - val_accuracy: 0.8534\n",
      "Epoch 123/300\n",
      "37356/37356 - 1s - loss: 0.0892 - accuracy: 0.8827 - val_loss: 0.1135 - val_accuracy: 0.8488\n",
      "Epoch 124/300\n",
      "37356/37356 - 1s - loss: 0.0885 - accuracy: 0.8837 - val_loss: 0.1105 - val_accuracy: 0.8526\n",
      "Epoch 125/300\n",
      "37356/37356 - 1s - loss: 0.0891 - accuracy: 0.8827 - val_loss: 0.1099 - val_accuracy: 0.8520\n",
      "Epoch 126/300\n",
      "37356/37356 - 1s - loss: 0.0894 - accuracy: 0.8817 - val_loss: 0.1105 - val_accuracy: 0.8563\n",
      "Epoch 127/300\n",
      "37356/37356 - 1s - loss: 0.0877 - accuracy: 0.8850 - val_loss: 0.1070 - val_accuracy: 0.8586\n",
      "Epoch 128/300\n",
      "37356/37356 - 1s - loss: 0.0882 - accuracy: 0.8834 - val_loss: 0.1070 - val_accuracy: 0.8564\n",
      "Epoch 129/300\n",
      "37356/37356 - 1s - loss: 0.0885 - accuracy: 0.8839 - val_loss: 0.1065 - val_accuracy: 0.8595\n",
      "Epoch 130/300\n",
      "37356/37356 - 1s - loss: 0.0871 - accuracy: 0.8860 - val_loss: 0.1102 - val_accuracy: 0.8537\n",
      "Epoch 131/300\n",
      "37356/37356 - 1s - loss: 0.0875 - accuracy: 0.8858 - val_loss: 0.1085 - val_accuracy: 0.8558\n",
      "Epoch 132/300\n",
      "37356/37356 - 1s - loss: 0.0875 - accuracy: 0.8845 - val_loss: 0.1077 - val_accuracy: 0.8581\n",
      "Epoch 133/300\n",
      "37356/37356 - 1s - loss: 0.0864 - accuracy: 0.8872 - val_loss: 0.1081 - val_accuracy: 0.8577\n",
      "Epoch 134/300\n",
      "37356/37356 - 1s - loss: 0.0871 - accuracy: 0.8856 - val_loss: 0.1089 - val_accuracy: 0.8566\n",
      "Epoch 135/300\n",
      "37356/37356 - 1s - loss: 0.0877 - accuracy: 0.8851 - val_loss: 0.1064 - val_accuracy: 0.8579\n",
      "Epoch 136/300\n",
      "37356/37356 - 1s - loss: 0.0867 - accuracy: 0.8865 - val_loss: 0.1074 - val_accuracy: 0.8601\n",
      "Epoch 137/300\n",
      "37356/37356 - 1s - loss: 0.0870 - accuracy: 0.8856 - val_loss: 0.1094 - val_accuracy: 0.8550\n",
      "Epoch 138/300\n",
      "37356/37356 - 1s - loss: 0.0868 - accuracy: 0.8864 - val_loss: 0.1076 - val_accuracy: 0.8587\n",
      "Epoch 139/300\n",
      "37356/37356 - 1s - loss: 0.0862 - accuracy: 0.8868 - val_loss: 0.1084 - val_accuracy: 0.8547\n",
      "Epoch 140/300\n",
      "37356/37356 - 1s - loss: 0.0859 - accuracy: 0.8881 - val_loss: 0.1072 - val_accuracy: 0.8599\n",
      "Epoch 141/300\n",
      "37356/37356 - 1s - loss: 0.0860 - accuracy: 0.8870 - val_loss: 0.1089 - val_accuracy: 0.8559\n",
      "Epoch 142/300\n",
      "37356/37356 - 1s - loss: 0.0863 - accuracy: 0.8871 - val_loss: 0.1081 - val_accuracy: 0.8550\n",
      "Epoch 143/300\n",
      "37356/37356 - 1s - loss: 0.0859 - accuracy: 0.8875 - val_loss: 0.1082 - val_accuracy: 0.8574\n",
      "Epoch 144/300\n",
      "37356/37356 - 1s - loss: 0.0864 - accuracy: 0.8874 - val_loss: 0.1066 - val_accuracy: 0.8587\n",
      "Epoch 145/300\n",
      "37356/37356 - 1s - loss: 0.0854 - accuracy: 0.8887 - val_loss: 0.1112 - val_accuracy: 0.8537\n",
      "Epoch 146/300\n",
      "37356/37356 - 1s - loss: 0.0853 - accuracy: 0.8894 - val_loss: 0.1093 - val_accuracy: 0.8564\n",
      "Epoch 147/300\n",
      "37356/37356 - 1s - loss: 0.0848 - accuracy: 0.8898 - val_loss: 0.1068 - val_accuracy: 0.8608\n",
      "Epoch 148/300\n",
      "37356/37356 - 1s - loss: 0.0852 - accuracy: 0.8900 - val_loss: 0.1102 - val_accuracy: 0.8531\n",
      "Epoch 149/300\n",
      "37356/37356 - 1s - loss: 0.0854 - accuracy: 0.8881 - val_loss: 0.1075 - val_accuracy: 0.8588\n",
      "Epoch 150/300\n",
      "37356/37356 - 1s - loss: 0.0848 - accuracy: 0.8896 - val_loss: 0.1086 - val_accuracy: 0.8578\n",
      "Epoch 151/300\n",
      "37356/37356 - 1s - loss: 0.0843 - accuracy: 0.8911 - val_loss: 0.1122 - val_accuracy: 0.8525\n",
      "Epoch 152/300\n",
      "37356/37356 - 1s - loss: 0.0841 - accuracy: 0.8902 - val_loss: 0.1094 - val_accuracy: 0.8570\n",
      "Epoch 153/300\n",
      "37356/37356 - 1s - loss: 0.0841 - accuracy: 0.8920 - val_loss: 0.1113 - val_accuracy: 0.8507\n",
      "Epoch 154/300\n",
      "37356/37356 - 1s - loss: 0.0835 - accuracy: 0.8908 - val_loss: 0.1096 - val_accuracy: 0.8538\n",
      "Epoch 155/300\n",
      "37356/37356 - 1s - loss: 0.0846 - accuracy: 0.8895 - val_loss: 0.1092 - val_accuracy: 0.8599\n",
      "Epoch 156/300\n",
      "37356/37356 - 1s - loss: 0.0840 - accuracy: 0.8912 - val_loss: 0.1072 - val_accuracy: 0.8583\n",
      "Epoch 157/300\n",
      "37356/37356 - 1s - loss: 0.0846 - accuracy: 0.8902 - val_loss: 0.1089 - val_accuracy: 0.8587\n",
      "Epoch 158/300\n",
      "37356/37356 - 1s - loss: 0.0836 - accuracy: 0.8925 - val_loss: 0.1092 - val_accuracy: 0.8570\n",
      "Epoch 159/300\n",
      "37356/37356 - 1s - loss: 0.0828 - accuracy: 0.8920 - val_loss: 0.1077 - val_accuracy: 0.8583\n",
      "Epoch 160/300\n",
      "37356/37356 - 1s - loss: 0.0830 - accuracy: 0.8932 - val_loss: 0.1080 - val_accuracy: 0.8579\n",
      "Epoch 161/300\n",
      "37356/37356 - 1s - loss: 0.0830 - accuracy: 0.8926 - val_loss: 0.1098 - val_accuracy: 0.8566\n",
      "Epoch 162/300\n",
      "37356/37356 - 1s - loss: 0.0823 - accuracy: 0.8931 - val_loss: 0.1084 - val_accuracy: 0.8564\n",
      "Epoch 163/300\n",
      "37356/37356 - 1s - loss: 0.0828 - accuracy: 0.8923 - val_loss: 0.1076 - val_accuracy: 0.8599\n",
      "Epoch 164/300\n",
      "37356/37356 - 1s - loss: 0.0826 - accuracy: 0.8938 - val_loss: 0.1084 - val_accuracy: 0.8619\n",
      "Epoch 165/300\n",
      "37356/37356 - 1s - loss: 0.0817 - accuracy: 0.8934 - val_loss: 0.1086 - val_accuracy: 0.8595\n",
      "Epoch 166/300\n",
      "37356/37356 - 1s - loss: 0.0820 - accuracy: 0.8929 - val_loss: 0.1068 - val_accuracy: 0.8599\n",
      "Epoch 167/300\n",
      "37356/37356 - 1s - loss: 0.0831 - accuracy: 0.8917 - val_loss: 0.1069 - val_accuracy: 0.8597\n",
      "Epoch 168/300\n",
      "37356/37356 - 1s - loss: 0.0815 - accuracy: 0.8946 - val_loss: 0.1095 - val_accuracy: 0.8557\n",
      "Epoch 169/300\n",
      "37356/37356 - 1s - loss: 0.0814 - accuracy: 0.8948 - val_loss: 0.1080 - val_accuracy: 0.8596\n",
      "Epoch 170/300\n",
      "37356/37356 - 1s - loss: 0.0816 - accuracy: 0.8936 - val_loss: 0.1061 - val_accuracy: 0.8621\n",
      "Epoch 171/300\n",
      "37356/37356 - 1s - loss: 0.0804 - accuracy: 0.8963 - val_loss: 0.1094 - val_accuracy: 0.8562\n",
      "Epoch 172/300\n",
      "37356/37356 - 1s - loss: 0.0806 - accuracy: 0.8960 - val_loss: 0.1054 - val_accuracy: 0.8632\n",
      "Epoch 173/300\n",
      "37356/37356 - 1s - loss: 0.0820 - accuracy: 0.8942 - val_loss: 0.1074 - val_accuracy: 0.8622\n",
      "Epoch 174/300\n",
      "37356/37356 - 1s - loss: 0.0815 - accuracy: 0.8946 - val_loss: 0.1070 - val_accuracy: 0.8621\n",
      "Epoch 175/300\n",
      "37356/37356 - 1s - loss: 0.0806 - accuracy: 0.8959 - val_loss: 0.1089 - val_accuracy: 0.8575\n",
      "Epoch 176/300\n",
      "37356/37356 - 1s - loss: 0.0811 - accuracy: 0.8952 - val_loss: 0.1093 - val_accuracy: 0.8577\n",
      "Epoch 177/300\n",
      "37356/37356 - 1s - loss: 0.0803 - accuracy: 0.8946 - val_loss: 0.1086 - val_accuracy: 0.8576\n",
      "Epoch 178/300\n",
      "37356/37356 - 1s - loss: 0.0811 - accuracy: 0.8943 - val_loss: 0.1093 - val_accuracy: 0.8590\n",
      "Epoch 179/300\n",
      "37356/37356 - 1s - loss: 0.0810 - accuracy: 0.8945 - val_loss: 0.1095 - val_accuracy: 0.8566\n",
      "Epoch 180/300\n",
      "37356/37356 - 1s - loss: 0.0802 - accuracy: 0.8961 - val_loss: 0.1076 - val_accuracy: 0.8582\n",
      "Epoch 181/300\n",
      "37356/37356 - 1s - loss: 0.0798 - accuracy: 0.8972 - val_loss: 0.1082 - val_accuracy: 0.8603\n",
      "Epoch 182/300\n",
      "37356/37356 - 1s - loss: 0.0791 - accuracy: 0.8980 - val_loss: 0.1093 - val_accuracy: 0.8588\n",
      "Epoch 183/300\n",
      "37356/37356 - 1s - loss: 0.0802 - accuracy: 0.8961 - val_loss: 0.1086 - val_accuracy: 0.8595\n",
      "Epoch 184/300\n",
      "37356/37356 - 1s - loss: 0.0799 - accuracy: 0.8971 - val_loss: 0.1086 - val_accuracy: 0.8566\n",
      "Epoch 185/300\n",
      "37356/37356 - 1s - loss: 0.0797 - accuracy: 0.8966 - val_loss: 0.1095 - val_accuracy: 0.8578\n",
      "Epoch 186/300\n",
      "37356/37356 - 1s - loss: 0.0788 - accuracy: 0.8995 - val_loss: 0.1077 - val_accuracy: 0.8638\n",
      "Epoch 187/300\n",
      "37356/37356 - 1s - loss: 0.0797 - accuracy: 0.8980 - val_loss: 0.1083 - val_accuracy: 0.8563\n",
      "Epoch 188/300\n",
      "37356/37356 - 1s - loss: 0.0797 - accuracy: 0.8962 - val_loss: 0.1137 - val_accuracy: 0.8522\n",
      "Epoch 189/300\n",
      "37356/37356 - 1s - loss: 0.0793 - accuracy: 0.8989 - val_loss: 0.1093 - val_accuracy: 0.8578\n",
      "Epoch 190/300\n",
      "37356/37356 - 1s - loss: 0.0780 - accuracy: 0.8991 - val_loss: 0.1096 - val_accuracy: 0.8597\n",
      "Epoch 191/300\n",
      "37356/37356 - 1s - loss: 0.0791 - accuracy: 0.8983 - val_loss: 0.1099 - val_accuracy: 0.8571\n",
      "Epoch 192/300\n",
      "37356/37356 - 1s - loss: 0.0783 - accuracy: 0.9001 - val_loss: 0.1096 - val_accuracy: 0.8599\n",
      "Epoch 193/300\n",
      "37356/37356 - 1s - loss: 0.0796 - accuracy: 0.8984 - val_loss: 0.1079 - val_accuracy: 0.8624\n",
      "Epoch 194/300\n",
      "37356/37356 - 1s - loss: 0.0781 - accuracy: 0.8995 - val_loss: 0.1098 - val_accuracy: 0.8572\n",
      "Epoch 195/300\n",
      "37356/37356 - 1s - loss: 0.0780 - accuracy: 0.8997 - val_loss: 0.1073 - val_accuracy: 0.8605\n",
      "Epoch 196/300\n",
      "37356/37356 - 1s - loss: 0.0787 - accuracy: 0.8984 - val_loss: 0.1147 - val_accuracy: 0.8509\n",
      "Epoch 197/300\n",
      "37356/37356 - 1s - loss: 0.0782 - accuracy: 0.8993 - val_loss: 0.1080 - val_accuracy: 0.8607\n",
      "Epoch 198/300\n",
      "37356/37356 - 1s - loss: 0.0776 - accuracy: 0.8999 - val_loss: 0.1112 - val_accuracy: 0.8593\n",
      "Epoch 199/300\n",
      "37356/37356 - 1s - loss: 0.0773 - accuracy: 0.8998 - val_loss: 0.1094 - val_accuracy: 0.8587\n",
      "Epoch 200/300\n",
      "37356/37356 - 1s - loss: 0.0771 - accuracy: 0.9017 - val_loss: 0.1071 - val_accuracy: 0.8621\n",
      "Epoch 201/300\n",
      "37356/37356 - 1s - loss: 0.0774 - accuracy: 0.9011 - val_loss: 0.1101 - val_accuracy: 0.8584\n",
      "Epoch 202/300\n",
      "37356/37356 - 1s - loss: 0.0781 - accuracy: 0.9001 - val_loss: 0.1106 - val_accuracy: 0.8598\n",
      "Epoch 203/300\n",
      "37356/37356 - 1s - loss: 0.0769 - accuracy: 0.9020 - val_loss: 0.1087 - val_accuracy: 0.8612\n",
      "Epoch 204/300\n",
      "37356/37356 - 1s - loss: 0.0768 - accuracy: 0.9014 - val_loss: 0.1102 - val_accuracy: 0.8571\n",
      "Epoch 205/300\n",
      "37356/37356 - 1s - loss: 0.0760 - accuracy: 0.9034 - val_loss: 0.1090 - val_accuracy: 0.8596\n",
      "Epoch 206/300\n",
      "37356/37356 - 1s - loss: 0.0762 - accuracy: 0.9035 - val_loss: 0.1079 - val_accuracy: 0.8619\n",
      "Epoch 207/300\n",
      "37356/37356 - 1s - loss: 0.0773 - accuracy: 0.9019 - val_loss: 0.1078 - val_accuracy: 0.8593\n",
      "Epoch 208/300\n",
      "37356/37356 - 1s - loss: 0.0770 - accuracy: 0.9019 - val_loss: 0.1072 - val_accuracy: 0.8634\n",
      "Epoch 209/300\n",
      "37356/37356 - 1s - loss: 0.0764 - accuracy: 0.9024 - val_loss: 0.1091 - val_accuracy: 0.8601\n",
      "Epoch 210/300\n",
      "37356/37356 - 1s - loss: 0.0754 - accuracy: 0.9040 - val_loss: 0.1085 - val_accuracy: 0.8600\n",
      "Epoch 211/300\n",
      "37356/37356 - 1s - loss: 0.0761 - accuracy: 0.9022 - val_loss: 0.1088 - val_accuracy: 0.8578\n",
      "Epoch 212/300\n",
      "37356/37356 - 1s - loss: 0.0761 - accuracy: 0.9027 - val_loss: 0.1109 - val_accuracy: 0.8563\n",
      "Epoch 213/300\n",
      "37356/37356 - 1s - loss: 0.0763 - accuracy: 0.9023 - val_loss: 0.1112 - val_accuracy: 0.8570\n",
      "Epoch 214/300\n",
      "37356/37356 - 1s - loss: 0.0764 - accuracy: 0.9023 - val_loss: 0.1105 - val_accuracy: 0.8568\n",
      "Epoch 215/300\n",
      "37356/37356 - 1s - loss: 0.0748 - accuracy: 0.9044 - val_loss: 0.1132 - val_accuracy: 0.8532\n",
      "Epoch 216/300\n",
      "37356/37356 - 1s - loss: 0.0758 - accuracy: 0.9038 - val_loss: 0.1092 - val_accuracy: 0.8588\n",
      "Epoch 217/300\n",
      "37356/37356 - 1s - loss: 0.0751 - accuracy: 0.9042 - val_loss: 0.1109 - val_accuracy: 0.8552\n",
      "Epoch 218/300\n",
      "37356/37356 - 1s - loss: 0.0748 - accuracy: 0.9055 - val_loss: 0.1090 - val_accuracy: 0.8603\n",
      "Epoch 219/300\n",
      "37356/37356 - 1s - loss: 0.0752 - accuracy: 0.9045 - val_loss: 0.1127 - val_accuracy: 0.8554\n",
      "Epoch 220/300\n",
      "37356/37356 - 1s - loss: 0.0753 - accuracy: 0.9042 - val_loss: 0.1097 - val_accuracy: 0.8591\n",
      "Epoch 221/300\n",
      "37356/37356 - 1s - loss: 0.0745 - accuracy: 0.9047 - val_loss: 0.1096 - val_accuracy: 0.8598\n",
      "Epoch 222/300\n",
      "37356/37356 - 1s - loss: 0.0741 - accuracy: 0.9059 - val_loss: 0.1115 - val_accuracy: 0.8570\n",
      "Epoch 223/300\n",
      "37356/37356 - 1s - loss: 0.0745 - accuracy: 0.9041 - val_loss: 0.1104 - val_accuracy: 0.8570\n",
      "Epoch 224/300\n",
      "37356/37356 - 1s - loss: 0.0752 - accuracy: 0.9037 - val_loss: 0.1126 - val_accuracy: 0.8580\n",
      "Epoch 225/300\n",
      "37356/37356 - 1s - loss: 0.0749 - accuracy: 0.9044 - val_loss: 0.1088 - val_accuracy: 0.8607\n",
      "Epoch 226/300\n",
      "37356/37356 - 1s - loss: 0.0741 - accuracy: 0.9056 - val_loss: 0.1093 - val_accuracy: 0.8591\n",
      "Epoch 227/300\n",
      "37356/37356 - 1s - loss: 0.0739 - accuracy: 0.9063 - val_loss: 0.1096 - val_accuracy: 0.8612\n",
      "Epoch 228/300\n",
      "37356/37356 - 1s - loss: 0.0747 - accuracy: 0.9052 - val_loss: 0.1087 - val_accuracy: 0.8575\n",
      "Epoch 229/300\n",
      "37356/37356 - 1s - loss: 0.0739 - accuracy: 0.9056 - val_loss: 0.1138 - val_accuracy: 0.8511\n",
      "Epoch 230/300\n",
      "37356/37356 - 1s - loss: 0.0731 - accuracy: 0.9088 - val_loss: 0.1102 - val_accuracy: 0.8558\n",
      "Epoch 231/300\n",
      "37356/37356 - 1s - loss: 0.0736 - accuracy: 0.9067 - val_loss: 0.1110 - val_accuracy: 0.8589\n",
      "Epoch 232/300\n",
      "37356/37356 - 1s - loss: 0.0734 - accuracy: 0.9071 - val_loss: 0.1131 - val_accuracy: 0.8528\n",
      "Epoch 233/300\n",
      "37356/37356 - 1s - loss: 0.0734 - accuracy: 0.9056 - val_loss: 0.1102 - val_accuracy: 0.8605\n",
      "Epoch 234/300\n",
      "37356/37356 - 1s - loss: 0.0735 - accuracy: 0.9064 - val_loss: 0.1105 - val_accuracy: 0.8579\n",
      "Epoch 235/300\n",
      "37356/37356 - 1s - loss: 0.0742 - accuracy: 0.9050 - val_loss: 0.1107 - val_accuracy: 0.8571\n",
      "Epoch 236/300\n",
      "37356/37356 - 1s - loss: 0.0732 - accuracy: 0.9072 - val_loss: 0.1098 - val_accuracy: 0.8604\n",
      "Epoch 237/300\n",
      "37356/37356 - 1s - loss: 0.0728 - accuracy: 0.9079 - val_loss: 0.1102 - val_accuracy: 0.8585\n",
      "Epoch 238/300\n",
      "37356/37356 - 1s - loss: 0.0727 - accuracy: 0.9072 - val_loss: 0.1089 - val_accuracy: 0.8605\n",
      "Epoch 239/300\n",
      "37356/37356 - 1s - loss: 0.0729 - accuracy: 0.9080 - val_loss: 0.1093 - val_accuracy: 0.8613\n",
      "Epoch 240/300\n",
      "37356/37356 - 1s - loss: 0.0737 - accuracy: 0.9065 - val_loss: 0.1120 - val_accuracy: 0.8553\n",
      "Epoch 241/300\n",
      "37356/37356 - 1s - loss: 0.0729 - accuracy: 0.9082 - val_loss: 0.1111 - val_accuracy: 0.8575\n",
      "Epoch 242/300\n",
      "37356/37356 - 1s - loss: 0.0721 - accuracy: 0.9098 - val_loss: 0.1119 - val_accuracy: 0.8556\n",
      "Epoch 243/300\n",
      "37356/37356 - 1s - loss: 0.0729 - accuracy: 0.9077 - val_loss: 0.1117 - val_accuracy: 0.8563\n",
      "Epoch 244/300\n",
      "37356/37356 - 1s - loss: 0.0736 - accuracy: 0.9068 - val_loss: 0.1161 - val_accuracy: 0.8506\n",
      "Epoch 245/300\n",
      "37356/37356 - 1s - loss: 0.0723 - accuracy: 0.9088 - val_loss: 0.1143 - val_accuracy: 0.8524\n",
      "Epoch 246/300\n",
      "37356/37356 - 1s - loss: 0.0719 - accuracy: 0.9082 - val_loss: 0.1079 - val_accuracy: 0.8616\n",
      "Epoch 247/300\n",
      "37356/37356 - 1s - loss: 0.0719 - accuracy: 0.9090 - val_loss: 0.1091 - val_accuracy: 0.8607\n",
      "Epoch 248/300\n",
      "37356/37356 - 1s - loss: 0.0733 - accuracy: 0.9063 - val_loss: 0.1126 - val_accuracy: 0.8522\n",
      "Epoch 249/300\n",
      "37356/37356 - 1s - loss: 0.0731 - accuracy: 0.9080 - val_loss: 0.1108 - val_accuracy: 0.8580\n",
      "Epoch 250/300\n",
      "37356/37356 - 1s - loss: 0.0713 - accuracy: 0.9107 - val_loss: 0.1104 - val_accuracy: 0.8599\n",
      "Epoch 251/300\n",
      "37356/37356 - 1s - loss: 0.0711 - accuracy: 0.9102 - val_loss: 0.1108 - val_accuracy: 0.8578\n",
      "Epoch 252/300\n",
      "37356/37356 - 1s - loss: 0.0713 - accuracy: 0.9104 - val_loss: 0.1113 - val_accuracy: 0.8602\n",
      "Epoch 253/300\n",
      "37356/37356 - 1s - loss: 0.0715 - accuracy: 0.9099 - val_loss: 0.1110 - val_accuracy: 0.8589\n",
      "Epoch 254/300\n",
      "37356/37356 - 1s - loss: 0.0722 - accuracy: 0.9087 - val_loss: 0.1088 - val_accuracy: 0.8607\n",
      "Epoch 255/300\n",
      "37356/37356 - 1s - loss: 0.0708 - accuracy: 0.9104 - val_loss: 0.1102 - val_accuracy: 0.8591\n",
      "Epoch 256/300\n",
      "37356/37356 - 1s - loss: 0.0705 - accuracy: 0.9112 - val_loss: 0.1113 - val_accuracy: 0.8599\n",
      "Epoch 257/300\n",
      "37356/37356 - 1s - loss: 0.0714 - accuracy: 0.9096 - val_loss: 0.1092 - val_accuracy: 0.8605\n",
      "Epoch 258/300\n",
      "37356/37356 - 1s - loss: 0.0720 - accuracy: 0.9088 - val_loss: 0.1129 - val_accuracy: 0.8561\n",
      "Epoch 259/300\n",
      "37356/37356 - 1s - loss: 0.0721 - accuracy: 0.9075 - val_loss: 0.1120 - val_accuracy: 0.8573\n",
      "Epoch 260/300\n",
      "37356/37356 - 1s - loss: 0.0704 - accuracy: 0.9114 - val_loss: 0.1117 - val_accuracy: 0.8567\n",
      "Epoch 261/300\n",
      "37356/37356 - 1s - loss: 0.0704 - accuracy: 0.9106 - val_loss: 0.1096 - val_accuracy: 0.8596\n",
      "Epoch 262/300\n",
      "37356/37356 - 1s - loss: 0.0705 - accuracy: 0.9108 - val_loss: 0.1112 - val_accuracy: 0.8578\n",
      "Epoch 263/300\n",
      "37356/37356 - 1s - loss: 0.0698 - accuracy: 0.9120 - val_loss: 0.1100 - val_accuracy: 0.8611\n",
      "Epoch 264/300\n",
      "37356/37356 - 1s - loss: 0.0705 - accuracy: 0.9106 - val_loss: 0.1116 - val_accuracy: 0.8575\n",
      "Epoch 265/300\n",
      "37356/37356 - 1s - loss: 0.0695 - accuracy: 0.9121 - val_loss: 0.1111 - val_accuracy: 0.8598\n",
      "Epoch 266/300\n",
      "37356/37356 - 1s - loss: 0.0700 - accuracy: 0.9125 - val_loss: 0.1111 - val_accuracy: 0.8583\n",
      "Epoch 267/300\n",
      "37356/37356 - 1s - loss: 0.0712 - accuracy: 0.9102 - val_loss: 0.1112 - val_accuracy: 0.8592\n",
      "Epoch 268/300\n",
      "37356/37356 - 1s - loss: 0.0701 - accuracy: 0.9127 - val_loss: 0.1117 - val_accuracy: 0.8583\n",
      "Epoch 269/300\n",
      "37356/37356 - 1s - loss: 0.0698 - accuracy: 0.9120 - val_loss: 0.1114 - val_accuracy: 0.8571\n",
      "Epoch 270/300\n",
      "37356/37356 - 1s - loss: 0.0699 - accuracy: 0.9123 - val_loss: 0.1126 - val_accuracy: 0.8584\n",
      "Epoch 271/300\n",
      "37356/37356 - 1s - loss: 0.0702 - accuracy: 0.9122 - val_loss: 0.1210 - val_accuracy: 0.8435\n",
      "Epoch 272/300\n",
      "37356/37356 - 1s - loss: 0.0712 - accuracy: 0.9103 - val_loss: 0.1100 - val_accuracy: 0.8618\n",
      "Epoch 273/300\n",
      "37356/37356 - 1s - loss: 0.0691 - accuracy: 0.9128 - val_loss: 0.1128 - val_accuracy: 0.8563\n",
      "Epoch 274/300\n",
      "37356/37356 - 1s - loss: 0.0698 - accuracy: 0.9126 - val_loss: 0.1101 - val_accuracy: 0.8607\n",
      "Epoch 275/300\n",
      "37356/37356 - 1s - loss: 0.0695 - accuracy: 0.9136 - val_loss: 0.1140 - val_accuracy: 0.8551\n",
      "Epoch 276/300\n",
      "37356/37356 - 1s - loss: 0.0694 - accuracy: 0.9132 - val_loss: 0.1107 - val_accuracy: 0.8580\n",
      "Epoch 277/300\n",
      "37356/37356 - 1s - loss: 0.0684 - accuracy: 0.9129 - val_loss: 0.1106 - val_accuracy: 0.8591\n",
      "Epoch 278/300\n",
      "37356/37356 - 1s - loss: 0.0692 - accuracy: 0.9131 - val_loss: 0.1153 - val_accuracy: 0.8523\n",
      "Epoch 279/300\n",
      "37356/37356 - 1s - loss: 0.0695 - accuracy: 0.9124 - val_loss: 0.1146 - val_accuracy: 0.8577\n",
      "Epoch 280/300\n",
      "37356/37356 - 1s - loss: 0.0694 - accuracy: 0.9135 - val_loss: 0.1169 - val_accuracy: 0.8513\n",
      "Epoch 281/300\n",
      "37356/37356 - 1s - loss: 0.0679 - accuracy: 0.9157 - val_loss: 0.1122 - val_accuracy: 0.8583\n",
      "Epoch 282/300\n",
      "37356/37356 - 1s - loss: 0.0698 - accuracy: 0.9121 - val_loss: 0.1121 - val_accuracy: 0.8577\n",
      "Epoch 283/300\n",
      "37356/37356 - 1s - loss: 0.0687 - accuracy: 0.9135 - val_loss: 0.1152 - val_accuracy: 0.8526\n",
      "Epoch 284/300\n",
      "37356/37356 - 1s - loss: 0.0687 - accuracy: 0.9135 - val_loss: 0.1138 - val_accuracy: 0.8557\n",
      "Epoch 285/300\n",
      "37356/37356 - 1s - loss: 0.0686 - accuracy: 0.9144 - val_loss: 0.1138 - val_accuracy: 0.8575\n",
      "Epoch 286/300\n",
      "37356/37356 - 1s - loss: 0.0688 - accuracy: 0.9131 - val_loss: 0.1129 - val_accuracy: 0.8581\n",
      "Epoch 287/300\n",
      "37356/37356 - 1s - loss: 0.0671 - accuracy: 0.9167 - val_loss: 0.1157 - val_accuracy: 0.8516\n",
      "Epoch 288/300\n",
      "37356/37356 - 1s - loss: 0.0682 - accuracy: 0.9154 - val_loss: 0.1129 - val_accuracy: 0.8579\n",
      "Epoch 289/300\n",
      "37356/37356 - 1s - loss: 0.0679 - accuracy: 0.9148 - val_loss: 0.1162 - val_accuracy: 0.8514\n",
      "Epoch 290/300\n",
      "37356/37356 - 1s - loss: 0.0687 - accuracy: 0.9142 - val_loss: 0.1117 - val_accuracy: 0.8622\n",
      "Epoch 291/300\n",
      "37356/37356 - 1s - loss: 0.0675 - accuracy: 0.9162 - val_loss: 0.1155 - val_accuracy: 0.8534\n",
      "Epoch 292/300\n",
      "37356/37356 - 1s - loss: 0.0681 - accuracy: 0.9142 - val_loss: 0.1151 - val_accuracy: 0.8596\n",
      "Epoch 293/300\n",
      "37356/37356 - 1s - loss: 0.0680 - accuracy: 0.9157 - val_loss: 0.1126 - val_accuracy: 0.8599\n",
      "Epoch 294/300\n",
      "37356/37356 - 1s - loss: 0.0680 - accuracy: 0.9146 - val_loss: 0.1139 - val_accuracy: 0.8557\n",
      "Epoch 295/300\n",
      "37356/37356 - 1s - loss: 0.0677 - accuracy: 0.9157 - val_loss: 0.1140 - val_accuracy: 0.8532\n",
      "Epoch 296/300\n",
      "37356/37356 - 1s - loss: 0.0674 - accuracy: 0.9170 - val_loss: 0.1151 - val_accuracy: 0.8547\n",
      "Epoch 297/300\n",
      "37356/37356 - 1s - loss: 0.0680 - accuracy: 0.9153 - val_loss: 0.1127 - val_accuracy: 0.8570\n",
      "Epoch 298/300\n",
      "37356/37356 - 1s - loss: 0.0676 - accuracy: 0.9155 - val_loss: 0.1159 - val_accuracy: 0.8526\n",
      "Epoch 299/300\n",
      "37356/37356 - 1s - loss: 0.0669 - accuracy: 0.9158 - val_loss: 0.1160 - val_accuracy: 0.8502\n",
      "Epoch 300/300\n",
      "37356/37356 - 1s - loss: 0.0687 - accuracy: 0.9131 - val_loss: 0.1128 - val_accuracy: 0.8570\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=300, batch_size=128, validation_data=(x_val, y_val), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
