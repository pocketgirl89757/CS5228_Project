{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 5228\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Imports\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import locale\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# model training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# classifiers\n",
    "from sklearn.naive_bayes import GaussianNB # naive bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN\n",
    "from sklearn.linear_model import LogisticRegression # logistic regression\n",
    "from sklearn.tree import DecisionTreeClassifier # decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "locale.setlocale(locale.LC_ALL,'')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Training Data\n",
    "drop_columns = ['CreateJob','RetainedJob','City','Name','Zip','BankState']\n",
    "\n",
    "# drop_columns = []\n",
    "\n",
    "le = generate_labels()\n",
    "\n",
    "base_dropna = get_data(le=le,type='train', dropna=True, get_dummy=True, feature_split=False, values_only=True,drop_columns=drop_columns)\n",
    "# base_fillna = get_data(le=le,type='train', dropna=False, get_dummy=True, feature_split=False, values_only=True,drop_columns=drop_columns)\n",
    "# feature_dropna = get_data(le=le,type='train', dropna=True, get_dummy=True, feature_split=True, values_only=True,drop_columns=drop_columns)\n",
    "# feature_fillna = get_data(le=le,type='train', dropna=False, get_dummy=True, feature_split=True, values_only=True,drop_columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 49808 entries, 0 to 49999\n",
      "Data columns (total 22 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   State              49808 non-null  int32  \n",
      " 1   Bank               49808 non-null  int32  \n",
      " 2   NAICS              49808 non-null  int32  \n",
      " 3   ApprovalDate       49808 non-null  int64  \n",
      " 4   ApprovalFY         49808 non-null  int16  \n",
      " 5   Term               49808 non-null  int64  \n",
      " 6   NoEmp              49808 non-null  int64  \n",
      " 7   FranchiseCode      49808 non-null  int32  \n",
      " 8   DisbursementDate   49808 non-null  int64  \n",
      " 9   DisbursementGross  49808 non-null  float32\n",
      " 10  GrAppv             49808 non-null  float32\n",
      " 11  SBA_Appv           49808 non-null  float32\n",
      " 12  ChargeOff          49808 non-null  int64  \n",
      " 13  NewExist_1         49808 non-null  uint8  \n",
      " 14  NewExist_2         49808 non-null  uint8  \n",
      " 15  UrbanRural_0       49808 non-null  uint8  \n",
      " 16  UrbanRural_1       49808 non-null  uint8  \n",
      " 17  UrbanRural_2       49808 non-null  uint8  \n",
      " 18  RevLineCr_N        49808 non-null  uint8  \n",
      " 19  RevLineCr_Y        49808 non-null  uint8  \n",
      " 20  LowDoc_N           49808 non-null  uint8  \n",
      " 21  LowDoc_Y           49808 non-null  uint8  \n",
      "dtypes: float32(3), int16(1), int32(4), int64(5), uint8(9)\n",
      "memory usage: 4.1 MB\n"
     ]
    }
   ],
   "source": [
    "base_dropna.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Test Data\n",
    "# feature_test = get_data(le=le,type='test', dropna=False, get_dummy=True, feature_split=True, values_only=True,drop_columns=drop_columns)\n",
    "base_test = get_data(le=le,type='test', dropna=False, get_dummy=True, feature_split=False, values_only=True,drop_columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Bank</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>ApprovalDate</th>\n",
       "      <th>ApprovalFY</th>\n",
       "      <th>FranchiseCode</th>\n",
       "      <th>DisbursementDate</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>NewExist_1</th>\n",
       "      <th>NewExist_2</th>\n",
       "      <th>UrbanRural_0</th>\n",
       "      <th>UrbanRural_1</th>\n",
       "      <th>UrbanRural_2</th>\n",
       "      <th>RevLineCr_N</th>\n",
       "      <th>RevLineCr_Y</th>\n",
       "      <th>LowDoc_N</th>\n",
       "      <th>LowDoc_Y</th>\n",
       "      <th>NoEmp_Micro</th>\n",
       "      <th>NoEmp_Small</th>\n",
       "      <th>NoEmp_Medium</th>\n",
       "      <th>NoEmp_Large</th>\n",
       "      <th>Term_Short</th>\n",
       "      <th>Term_Intermediate</th>\n",
       "      <th>Term_Long</th>\n",
       "      <th>Term_Extra Long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.311220</td>\n",
       "      <td>1872.726350</td>\n",
       "      <td>421790.956760</td>\n",
       "      <td>1.023636e+09</td>\n",
       "      <td>2002.426590</td>\n",
       "      <td>2595.417730</td>\n",
       "      <td>1.033001e+09</td>\n",
       "      <td>1.750579e+05</td>\n",
       "      <td>1.647051e+05</td>\n",
       "      <td>1.244969e+05</td>\n",
       "      <td>0.708940</td>\n",
       "      <td>0.289900</td>\n",
       "      <td>0.275280</td>\n",
       "      <td>0.60408</td>\n",
       "      <td>0.12064</td>\n",
       "      <td>0.737600</td>\n",
       "      <td>0.262400</td>\n",
       "      <td>0.89246</td>\n",
       "      <td>0.099440</td>\n",
       "      <td>0.762720</td>\n",
       "      <td>0.210320</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.052240</td>\n",
       "      <td>0.12843</td>\n",
       "      <td>0.786920</td>\n",
       "      <td>0.032410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.134917</td>\n",
       "      <td>1325.081943</td>\n",
       "      <td>250716.950871</td>\n",
       "      <td>1.877301e+08</td>\n",
       "      <td>6.208961</td>\n",
       "      <td>12362.965686</td>\n",
       "      <td>1.865782e+08</td>\n",
       "      <td>2.691642e+05</td>\n",
       "      <td>2.652480e+05</td>\n",
       "      <td>2.096262e+05</td>\n",
       "      <td>0.454253</td>\n",
       "      <td>0.453718</td>\n",
       "      <td>0.446657</td>\n",
       "      <td>0.48905</td>\n",
       "      <td>0.32571</td>\n",
       "      <td>0.439941</td>\n",
       "      <td>0.439941</td>\n",
       "      <td>0.30980</td>\n",
       "      <td>0.299253</td>\n",
       "      <td>0.425417</td>\n",
       "      <td>0.407538</td>\n",
       "      <td>0.158688</td>\n",
       "      <td>0.033298</td>\n",
       "      <td>0.222512</td>\n",
       "      <td>0.33457</td>\n",
       "      <td>0.409486</td>\n",
       "      <td>0.177087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.978880e+07</td>\n",
       "      <td>1968.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.641920e+07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000e+02</td>\n",
       "      <td>2.000000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>574.000000</td>\n",
       "      <td>238220.000000</td>\n",
       "      <td>9.125568e+08</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.254304e+08</td>\n",
       "      <td>3.500000e+04</td>\n",
       "      <td>2.500000e+04</td>\n",
       "      <td>1.600000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>1892.500000</td>\n",
       "      <td>448210.000000</td>\n",
       "      <td>1.085443e+09</td>\n",
       "      <td>2004.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.093910e+09</td>\n",
       "      <td>8.100000e+04</td>\n",
       "      <td>6.500000e+04</td>\n",
       "      <td>4.300000e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>3017.000000</td>\n",
       "      <td>561790.000000</td>\n",
       "      <td>1.159942e+09</td>\n",
       "      <td>2007.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.164845e+09</td>\n",
       "      <td>1.940000e+05</td>\n",
       "      <td>1.750000e+05</td>\n",
       "      <td>1.330000e+05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>4015.000000</td>\n",
       "      <td>928120.000000</td>\n",
       "      <td>1.399853e+09</td>\n",
       "      <td>2017.000000</td>\n",
       "      <td>91350.000000</td>\n",
       "      <td>1.483229e+09</td>\n",
       "      <td>8.995000e+06</td>\n",
       "      <td>5.000000e+06</td>\n",
       "      <td>4.500000e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               State           Bank          NAICS  ApprovalDate  \\\n",
       "count  100000.000000  100000.000000  100000.000000  1.000000e+05   \n",
       "mean       24.311220    1872.726350  421790.956760  1.023636e+09   \n",
       "std        15.134917    1325.081943  250716.950871  1.877301e+08   \n",
       "min         0.000000       0.000000       0.000000 -5.978880e+07   \n",
       "25%        10.000000     574.000000  238220.000000  9.125568e+08   \n",
       "50%        24.000000    1892.500000  448210.000000  1.085443e+09   \n",
       "75%        37.000000    3017.000000  561790.000000  1.159942e+09   \n",
       "max        51.000000    4015.000000  928120.000000  1.399853e+09   \n",
       "\n",
       "          ApprovalFY  FranchiseCode  DisbursementDate  DisbursementGross  \\\n",
       "count  100000.000000  100000.000000      1.000000e+05       1.000000e+05   \n",
       "mean     2002.426590    2595.417730      1.033001e+09       1.750579e+05   \n",
       "std         6.208961   12362.965686      1.865782e+08       2.691642e+05   \n",
       "min      1968.000000       0.000000     -5.641920e+07       0.000000e+00   \n",
       "25%      1999.000000       0.000000      9.254304e+08       3.500000e+04   \n",
       "50%      2004.000000       0.000000      1.093910e+09       8.100000e+04   \n",
       "75%      2007.000000       0.000000      1.164845e+09       1.940000e+05   \n",
       "max      2017.000000   91350.000000      1.483229e+09       8.995000e+06   \n",
       "\n",
       "             GrAppv      SBA_Appv     NewExist_1     NewExist_2  \\\n",
       "count  1.000000e+05  1.000000e+05  100000.000000  100000.000000   \n",
       "mean   1.647051e+05  1.244969e+05       0.708940       0.289900   \n",
       "std    2.652480e+05  2.096262e+05       0.454253       0.453718   \n",
       "min    4.000000e+02  2.000000e+02       0.000000       0.000000   \n",
       "25%    2.500000e+04  1.600000e+04       0.000000       0.000000   \n",
       "50%    6.500000e+04  4.300000e+04       1.000000       0.000000   \n",
       "75%    1.750000e+05  1.330000e+05       1.000000       1.000000   \n",
       "max    5.000000e+06  4.500000e+06       1.000000       1.000000   \n",
       "\n",
       "        UrbanRural_0  UrbanRural_1  UrbanRural_2    RevLineCr_N  \\\n",
       "count  100000.000000  100000.00000  100000.00000  100000.000000   \n",
       "mean        0.275280       0.60408       0.12064       0.737600   \n",
       "std         0.446657       0.48905       0.32571       0.439941   \n",
       "min         0.000000       0.00000       0.00000       0.000000   \n",
       "25%         0.000000       0.00000       0.00000       0.000000   \n",
       "50%         0.000000       1.00000       0.00000       1.000000   \n",
       "75%         1.000000       1.00000       0.00000       1.000000   \n",
       "max         1.000000       1.00000       1.00000       1.000000   \n",
       "\n",
       "         RevLineCr_Y      LowDoc_N       LowDoc_Y    NoEmp_Micro  \\\n",
       "count  100000.000000  100000.00000  100000.000000  100000.000000   \n",
       "mean        0.262400       0.89246       0.099440       0.762720   \n",
       "std         0.439941       0.30980       0.299253       0.425417   \n",
       "min         0.000000       0.00000       0.000000       0.000000   \n",
       "25%         0.000000       1.00000       0.000000       1.000000   \n",
       "50%         0.000000       1.00000       0.000000       1.000000   \n",
       "75%         1.000000       1.00000       0.000000       1.000000   \n",
       "max         1.000000       1.00000       1.000000       1.000000   \n",
       "\n",
       "         NoEmp_Small   NoEmp_Medium    NoEmp_Large     Term_Short  \\\n",
       "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
       "mean        0.210320       0.025850       0.001110       0.052240   \n",
       "std         0.407538       0.158688       0.033298       0.222512   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       Term_Intermediate      Term_Long  Term_Extra Long  \n",
       "count       100000.00000  100000.000000    100000.000000  \n",
       "mean             0.12843       0.786920         0.032410  \n",
       "std              0.33457       0.409486         0.177087  \n",
       "min              0.00000       0.000000         0.000000  \n",
       "25%              0.00000       1.000000         0.000000  \n",
       "50%              0.00000       1.000000         0.000000  \n",
       "75%              0.00000       1.000000         0.000000  \n",
       "max              1.00000       1.000000         1.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Bank</th>\n",
       "      <th>BankState</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>ApprovalDate</th>\n",
       "      <th>ApprovalFY</th>\n",
       "      <th>CreateJob</th>\n",
       "      <th>RetainedJob</th>\n",
       "      <th>FranchiseCode</th>\n",
       "      <th>DisbursementDate</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>NewExist_1</th>\n",
       "      <th>NewExist_2</th>\n",
       "      <th>UrbanRural_0</th>\n",
       "      <th>UrbanRural_1</th>\n",
       "      <th>UrbanRural_2</th>\n",
       "      <th>RevLineCr_N</th>\n",
       "      <th>RevLineCr_Y</th>\n",
       "      <th>LowDoc_N</th>\n",
       "      <th>LowDoc_Y</th>\n",
       "      <th>NoEmp_Micro</th>\n",
       "      <th>NoEmp_Small</th>\n",
       "      <th>NoEmp_Medium</th>\n",
       "      <th>NoEmp_Large</th>\n",
       "      <th>Term_Short</th>\n",
       "      <th>Term_Intermediate</th>\n",
       "      <th>Term_Long</th>\n",
       "      <th>Term_Extra Long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19510</td>\n",
       "      <td>1344</td>\n",
       "      <td>35</td>\n",
       "      <td>11209</td>\n",
       "      <td>312</td>\n",
       "      <td>28</td>\n",
       "      <td>445110</td>\n",
       "      <td>1134604800</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1135987200</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119715</td>\n",
       "      <td>5077</td>\n",
       "      <td>4</td>\n",
       "      <td>85297</td>\n",
       "      <td>2137</td>\n",
       "      <td>41</td>\n",
       "      <td>722211</td>\n",
       "      <td>1051747200</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78760</td>\n",
       "      <td>1056931200</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>110500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>6800</td>\n",
       "      <td>44</td>\n",
       "      <td>77450</td>\n",
       "      <td>291</td>\n",
       "      <td>35</td>\n",
       "      <td>423120</td>\n",
       "      <td>1161043200</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>407</td>\n",
       "      <td>1167523200</td>\n",
       "      <td>184000.0</td>\n",
       "      <td>184000.0</td>\n",
       "      <td>138000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34680</td>\n",
       "      <td>6635</td>\n",
       "      <td>18</td>\n",
       "      <td>40337</td>\n",
       "      <td>1033</td>\n",
       "      <td>18</td>\n",
       "      <td>447110</td>\n",
       "      <td>973641600</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>988588800</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>101250.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125633</td>\n",
       "      <td>14103</td>\n",
       "      <td>36</td>\n",
       "      <td>44087</td>\n",
       "      <td>651</td>\n",
       "      <td>47</td>\n",
       "      <td>722110</td>\n",
       "      <td>1130371200</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1135987200</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name   City  State    Zip  Bank  BankState   NAICS  ApprovalDate  \\\n",
       "0   19510   1344     35  11209   312         28  445110    1134604800   \n",
       "1  119715   5077      4  85297  2137         41  722211    1051747200   \n",
       "2     104   6800     44  77450   291         35  423120    1161043200   \n",
       "3   34680   6635     18  40337  1033         18  447110     973641600   \n",
       "4  125633  14103     36  44087   651         47  722110    1130371200   \n",
       "\n",
       "   ApprovalFY  CreateJob  RetainedJob  FranchiseCode  DisbursementDate  \\\n",
       "0      2006.0          0            6              0        1135987200   \n",
       "1      2003.0          0            0          78760        1056931200   \n",
       "2      2007.0          1            1            407        1167523200   \n",
       "3      2001.0          0            0              0         988588800   \n",
       "4      2006.0          1            9              0        1135987200   \n",
       "\n",
       "   DisbursementGross    GrAppv  SBA_Appv  NewExist_1  NewExist_2  \\\n",
       "0           120000.0  100000.0   50000.0           1           0   \n",
       "1           130000.0  130000.0  110500.0           0           1   \n",
       "2           184000.0  184000.0  138000.0           0           1   \n",
       "3            80000.0  135000.0  101250.0           1           0   \n",
       "4            50000.0   50000.0   25000.0           1           0   \n",
       "\n",
       "   UrbanRural_0  UrbanRural_1  UrbanRural_2  RevLineCr_N  RevLineCr_Y  \\\n",
       "0             0             1             0            0            1   \n",
       "1             0             1             0            1            0   \n",
       "2             0             1             0            1            0   \n",
       "3             0             1             0            1            0   \n",
       "4             0             1             0            1            0   \n",
       "\n",
       "   LowDoc_N  LowDoc_Y  NoEmp_Micro  NoEmp_Small  NoEmp_Medium  NoEmp_Large  \\\n",
       "0         1         0            1            0             0            0   \n",
       "1         0         1            0            1             0            0   \n",
       "2         1         0            1            0             0            0   \n",
       "3         1         0            1            0             0            0   \n",
       "4         1         0            1            0             0            0   \n",
       "\n",
       "   Term_Short  Term_Intermediate  Term_Long  Term_Extra Long  \n",
       "0           0                  0          1                0  \n",
       "1           0                  0          1                0  \n",
       "2           0                  0          1                0  \n",
       "3           0                  0          1                0  \n",
       "4           1                  0          0                0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['KNN', 'LR', 'DT', 'RF', 'GBM']\n",
    "base_dropna_f1 = []\n",
    "base_dropna_acc = []\n",
    "base_fillna_f1 = []\n",
    "base_fillna_acc = []\n",
    "feature_dropna_f1 = []\n",
    "feature_dropna_acc = []\n",
    "feature_fillna_f1 = []\n",
    "feature_fillna_acc = []\n",
    "\n",
    "def calculate_acc_and_f1(classifier, x_train, y_train, x_test, y_test):\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    f1 = round(f1_score(y_test, y_pred, average='weighted') * 100, 2)\n",
    "    acc = round(accuracy_score(y_test, y_pred) * 100, 2)\n",
    "    return f1, acc\n",
    "\n",
    "    \n",
    "def train_single_classifier(classifier, df_in, f1_list, acc_list):\n",
    "    df_x = df_in.drop(columns='ChargeOff')\n",
    "    df_y = df_in['ChargeOff']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size = 0.25, random_state=0)\n",
    "    f1, acc = calculate_acc_and_f1(classifier, x_train, y_train, x_test, y_test)\n",
    "    f1_list.append(f1)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "\n",
    "def train_model(df_in, f1_list, acc_list):\n",
    "    train_single_classifier(KNeighborsClassifier(), df_in, f1_list, acc_list)\n",
    "    train_single_classifier(LogisticRegression(), df_in, f1_list, acc_list)\n",
    "    train_single_classifier(DecisionTreeClassifier(), df_in, f1_list, acc_list)\n",
    "    train_single_classifier(RandomForestClassifier(), df_in, f1_list, acc_list)\n",
    "    train_single_classifier(GradientBoostingClassifier(), df_in, f1_list, acc_list)\n",
    "    \n",
    "\n",
    "train_model(base_dropna, base_dropna_f1, base_dropna_acc)\n",
    "train_model(base_fillna, base_fillna_f1, base_fillna_acc)\n",
    "train_model(feature_dropna, feature_dropna_f1, feature_dropna_acc)\n",
    "train_model(feature_fillna, feature_fillna_f1, feature_fillna_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       base_dropna_acc  base_fillna_acc  feature_dropna_acc  \\\n",
      "Model                                                         \n",
      "KNN              68.50           68.460              68.500   \n",
      "LR               63.76           61.220              63.750   \n",
      "DT               87.78           87.260              71.790   \n",
      "RF               90.21           89.850              79.510   \n",
      "GBM              90.10           89.590              77.680   \n",
      "avg              80.07           79.276              72.246   \n",
      "\n",
      "       feature_fillna_acc  acc_mean  \n",
      "Model                                \n",
      "KNN                68.460    68.480  \n",
      "LR                 61.220    62.490  \n",
      "DT                 71.830    79.670  \n",
      "RF                 78.950    84.630  \n",
      "GBM                77.870    83.810  \n",
      "avg                71.666    75.816  \n",
      "\n",
      "\n",
      "       base_dropna_f1  base_fillna_f1  feature_dropna_f1  feature_fillna_f1  \\\n",
      "Model                                                                         \n",
      "KNN            68.490          68.460             68.490             68.460   \n",
      "LR             63.030          59.950             63.010             59.950   \n",
      "DT             87.780          87.250             71.790             71.830   \n",
      "RF             90.210          89.840             79.490             78.910   \n",
      "GBM            90.100          89.590             77.680             77.860   \n",
      "avg            79.922          79.018             72.092             71.402   \n",
      "\n",
      "       F1_mean  \n",
      "Model           \n",
      "KNN     68.470  \n",
      "LR      61.480  \n",
      "DT      79.660  \n",
      "RF      84.610  \n",
      "GBM     83.810  \n",
      "avg     75.606  \n"
     ]
    }
   ],
   "source": [
    "accuracy_record = pd.DataFrame({'Model': model_names, 'base_dropna_acc': base_dropna_acc, 'base_fillna_acc': base_fillna_acc, 'feature_dropna_acc': feature_dropna_acc, 'feature_fillna_acc': feature_fillna_acc})\n",
    "# accuracy_record = pd.DataFrame({'Model': model_names, 'base_dropna_acc': base_dropna_acc, 'feature_dropna_acc': feature_dropna_acc})\n",
    "accuracy_record['acc_mean'] = accuracy_record.mean(axis=1).round(2)\n",
    "accuracy_record.set_index('Model', inplace=True)\n",
    "accuracy_record.loc['avg'] = accuracy_record.mean()\n",
    "\n",
    "F1_record = pd.DataFrame({'Model': model_names, 'base_dropna_f1': base_dropna_f1, 'base_fillna_f1': base_fillna_f1, 'feature_dropna_f1': feature_dropna_f1, 'feature_fillna_f1': feature_fillna_f1})\n",
    "# F1_record = pd.DataFrame({'Model': model_names, 'base_dropna_f1': base_dropna_f1, 'feature_dropna_f1': feature_dropna_f1})\n",
    "F1_record['F1_mean'] = F1_record.mean(axis=1).round(2)\n",
    "F1_record.set_index('Model', inplace=True)\n",
    "F1_record.loc['avg'] = F1_record.mean()\n",
    "\n",
    "print(accuracy_record)\n",
    "print('\\n')\n",
    "print(F1_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "base_dropna_x = base_dropna.drop(columns='ChargeOff')\n",
    "base_dropna_y = base_dropna['ChargeOff']\n",
    "model.fit(base_dropna_x, base_dropna_y)\n",
    "test_pred = model.predict(base_test)\n",
    "pd.DataFrame(test_pred).to_csv('y_pred.csv',header=['ChargeOff'],index_label=\"Id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_grid: \n",
      " {'loss': ['deviance', 'exponential'], 'learning_rate': [0.05, 0.1, 0.3], 'n_estimators': [50, 100, 150], 'subsample': [0.9, 1], 'criterion': ['friedman_mse', 'mse', 'mae']}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   40.1s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   47.8s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  5.3min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  8.3min\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(base_dropna_x, base_dropna_y, test_size = 0.25, random_state=0)\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "# c = np.append(np.logspace(0, 4, 20),[0.001,.009,0.01,.09,1,5,10,25,100])\n",
    "# param_grid = {'loss': ['deviance', 'exponential'],\n",
    "#               'learning_rate': [0.001,0.05,0.1,0.2,0.5],\n",
    "#               'n_estimators':[50,100,200,500,1000],\n",
    "#               'subsample':[0.9,1],\n",
    "#               'criterion':['friedman_mse', 'mse', 'mae'],\n",
    "#               'min_samples_split':[2,5,10]\n",
    "#              }\n",
    "\n",
    "param_grid = {'loss': ['deviance', 'exponential'],\n",
    "              'learning_rate': [0.05,0.1,0.3],\n",
    "              'n_estimators':[50,100,150],\n",
    "              'subsample':[0.9,1],\n",
    "              'criterion':['friedman_mse', 'mse', 'mae'],\n",
    "#               'min_samples_split':[2,5],\n",
    "#               'max_depth':[3,5,7],\n",
    "#               'max_features':['sqrt','log2', None],\n",
    "              \n",
    "             }\n",
    "print('param_grid: \\n',param_grid)\n",
    "\n",
    "lr_cv = GridSearchCV(clf, param_grid,scoring = 'accuracy',verbose=10,n_jobs=-1)\n",
    "lr_cv.fit(x_train, y_train)\n",
    "\n",
    "#Predict values based on new parameters\n",
    "# y_pred_acc = lr_cv.predict(x_test)\n",
    "\n",
    "print(\"Best Parameters\",lr_cv.best_params_)\n",
    "print(\"Best Accuracy :\",lr_cv.best_score_)\n",
    "\n",
    "y_pred= lr_cv.predict(x_test)\n",
    "print(\"Accuracy: \",round(accuracy_score(y_test, y_pred) * 100, 2))\n",
    "print('Weighted F1 Mesure: ',round(f1_score(y_test, y_pred, average='weighted') * 100, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = lr_cv.predict(base_test)\n",
    "pd.DataFrame(test_pred).to_csv('y_pred_grid_search.csv',header=['ChargeOff'],index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Bank</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>ApprovalDate</th>\n",
       "      <th>ApprovalFY</th>\n",
       "      <th>Term</th>\n",
       "      <th>NoEmp</th>\n",
       "      <th>FranchiseCode</th>\n",
       "      <th>DisbursementDate</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>NewExist_1</th>\n",
       "      <th>NewExist_2</th>\n",
       "      <th>UrbanRural_0</th>\n",
       "      <th>UrbanRural_1</th>\n",
       "      <th>UrbanRural_2</th>\n",
       "      <th>RevLineCr_N</th>\n",
       "      <th>RevLineCr_Y</th>\n",
       "      <th>LowDoc_N</th>\n",
       "      <th>LowDoc_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>1</td>\n",
       "      <td>1603</td>\n",
       "      <td>621210</td>\n",
       "      <td>700358400</td>\n",
       "      <td>1992</td>\n",
       "      <td>180</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>720489600</td>\n",
       "      <td>660000.0</td>\n",
       "      <td>660000.0</td>\n",
       "      <td>528000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>5</td>\n",
       "      <td>1036</td>\n",
       "      <td>447110</td>\n",
       "      <td>1007683200</td>\n",
       "      <td>2002</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1009756800</td>\n",
       "      <td>672000.0</td>\n",
       "      <td>672000.0</td>\n",
       "      <td>504000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42924</th>\n",
       "      <td>45</td>\n",
       "      <td>2218</td>\n",
       "      <td>423990</td>\n",
       "      <td>1149552000</td>\n",
       "      <td>2006</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1151625600</td>\n",
       "      <td>24803.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>12500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34293</th>\n",
       "      <td>29</td>\n",
       "      <td>3332</td>\n",
       "      <td>812320</td>\n",
       "      <td>691891200</td>\n",
       "      <td>1992</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>696816000</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>24300.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47616</th>\n",
       "      <td>5</td>\n",
       "      <td>312</td>\n",
       "      <td>334611</td>\n",
       "      <td>1172102400</td>\n",
       "      <td>2007</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1175299200</td>\n",
       "      <td>39000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>17500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       State  Bank   NAICS  ApprovalDate  ApprovalFY  Term  NoEmp  \\\n",
       "1524       1  1603  621210     700358400        1992   180      5   \n",
       "2282       5  1036  447110    1007683200        2002   300      3   \n",
       "42924     45  2218  423990    1149552000        2006    56      2   \n",
       "34293     29  3332  812320     691891200        1992    60      5   \n",
       "47616      5   312  334611    1172102400        2007    65      3   \n",
       "\n",
       "       FranchiseCode  DisbursementDate  DisbursementGross    GrAppv  SBA_Appv  \\\n",
       "1524               0         720489600           660000.0  660000.0  528000.0   \n",
       "2282               0        1009756800           672000.0  672000.0  504000.0   \n",
       "42924              0        1151625600            24803.0   25000.0   12500.0   \n",
       "34293              0         696816000            27000.0   27000.0   24300.0   \n",
       "47616              0        1175299200            39000.0   35000.0   17500.0   \n",
       "\n",
       "       NewExist_1  NewExist_2  UrbanRural_0  UrbanRural_1  UrbanRural_2  \\\n",
       "1524            1           0             1             0             0   \n",
       "2282            1           0             0             1             0   \n",
       "42924           1           0             0             1             0   \n",
       "34293           1           0             1             0             0   \n",
       "47616           1           0             0             1             0   \n",
       "\n",
       "       RevLineCr_N  RevLineCr_Y  LowDoc_N  LowDoc_Y  \n",
       "1524             1            0         1         0  \n",
       "2282             1            0         1         0  \n",
       "42924            1            0         1         0  \n",
       "34293            1            0         1         0  \n",
       "47616            0            1         1         0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60198904\n",
      "Iteration 2, loss = 0.50193618\n",
      "Iteration 3, loss = 0.48236843\n",
      "Iteration 4, loss = 0.47253681\n",
      "Iteration 5, loss = 0.45816044\n",
      "Iteration 6, loss = 0.45016718\n",
      "Iteration 7, loss = 0.44224519\n",
      "Iteration 8, loss = 0.43803295\n",
      "Iteration 9, loss = 0.43026498\n",
      "Iteration 10, loss = 0.42888666\n",
      "Iteration 11, loss = 0.42479179\n",
      "Iteration 12, loss = 0.42170115\n",
      "Iteration 13, loss = 0.42181479\n",
      "Iteration 14, loss = 0.41863879\n",
      "Iteration 15, loss = 0.41695460\n",
      "Iteration 16, loss = 0.41615414\n",
      "Iteration 17, loss = 0.41367839\n",
      "Iteration 18, loss = 0.41481205\n",
      "Iteration 19, loss = 0.41378864\n",
      "Iteration 20, loss = 0.41027218\n",
      "Iteration 21, loss = 0.41197950\n",
      "Iteration 22, loss = 0.40848270\n",
      "Iteration 23, loss = 0.40768836\n",
      "Iteration 24, loss = 0.40890082\n",
      "Iteration 25, loss = 0.40752006\n",
      "Iteration 26, loss = 0.40766812\n",
      "Iteration 27, loss = 0.40610868\n",
      "Iteration 28, loss = 0.40447840\n",
      "Iteration 29, loss = 0.40294470\n",
      "Iteration 30, loss = 0.40350696\n",
      "Iteration 31, loss = 0.40219535\n",
      "Iteration 32, loss = 0.40202391\n",
      "Iteration 33, loss = 0.39855190\n",
      "Iteration 34, loss = 0.39940510\n",
      "Iteration 35, loss = 0.39825808\n",
      "Iteration 36, loss = 0.39747645\n",
      "Iteration 37, loss = 0.39446861\n",
      "Iteration 38, loss = 0.39330360\n",
      "Iteration 39, loss = 0.39362183\n",
      "Iteration 40, loss = 0.39252475\n",
      "Iteration 41, loss = 0.39045265\n",
      "Iteration 42, loss = 0.39108715\n",
      "Iteration 43, loss = 0.38953118\n",
      "Iteration 44, loss = 0.38999434\n",
      "Iteration 45, loss = 0.39044952\n",
      "Iteration 46, loss = 0.38804065\n",
      "Iteration 47, loss = 0.38933550\n",
      "Iteration 48, loss = 0.38562569\n",
      "Iteration 49, loss = 0.38538688\n",
      "Iteration 50, loss = 0.38668597\n",
      "Iteration 51, loss = 0.38544769\n",
      "Iteration 52, loss = 0.38573562\n",
      "Iteration 53, loss = 0.38461487\n",
      "Iteration 54, loss = 0.38655208\n",
      "Iteration 55, loss = 0.38380423\n",
      "Iteration 56, loss = 0.38236242\n",
      "Iteration 57, loss = 0.38353570\n",
      "Iteration 58, loss = 0.38410788\n",
      "Iteration 59, loss = 0.38341345\n",
      "Iteration 60, loss = 0.38197502\n",
      "Iteration 61, loss = 0.38298535\n",
      "Iteration 62, loss = 0.38137591\n",
      "Iteration 63, loss = 0.38105037\n",
      "Iteration 64, loss = 0.38223580\n",
      "Iteration 65, loss = 0.38182370\n",
      "Iteration 66, loss = 0.38180804\n",
      "Iteration 67, loss = 0.38194269\n",
      "Iteration 68, loss = 0.38033131\n",
      "Iteration 69, loss = 0.38022825\n",
      "Iteration 70, loss = 0.38107074\n",
      "Iteration 71, loss = 0.38182343\n",
      "Iteration 72, loss = 0.38217353\n",
      "Iteration 73, loss = 0.38023751\n",
      "Iteration 74, loss = 0.38108811\n",
      "Iteration 75, loss = 0.38877046\n",
      "Iteration 76, loss = 0.37973863\n",
      "Iteration 77, loss = 0.38039019\n",
      "Iteration 78, loss = 0.37970769\n",
      "Iteration 79, loss = 0.38148673\n",
      "Iteration 80, loss = 0.37933648\n",
      "Iteration 81, loss = 0.37934597\n",
      "Iteration 82, loss = 0.37970526\n",
      "Iteration 83, loss = 0.38107830\n",
      "Iteration 84, loss = 0.38021245\n",
      "Iteration 85, loss = 0.38042388\n",
      "Iteration 86, loss = 0.37895152\n",
      "Iteration 87, loss = 0.37946596\n",
      "Iteration 88, loss = 0.37835479\n",
      "Iteration 89, loss = 0.37815679\n",
      "Iteration 90, loss = 0.37940537\n",
      "Iteration 91, loss = 0.37877750\n",
      "Iteration 92, loss = 0.37612232\n",
      "Iteration 93, loss = 0.37722938\n",
      "Iteration 94, loss = 0.37763046\n",
      "Iteration 95, loss = 0.37741852\n",
      "Iteration 96, loss = 0.37822650\n",
      "Iteration 97, loss = 0.37667129\n",
      "Iteration 98, loss = 0.37738482\n",
      "Iteration 99, loss = 0.37743601\n",
      "Iteration 100, loss = 0.37886422\n",
      "Iteration 101, loss = 0.37761388\n",
      "Iteration 102, loss = 0.37597801\n",
      "Iteration 103, loss = 0.37669205\n",
      "Iteration 104, loss = 0.37836031\n",
      "Iteration 105, loss = 0.38078772\n",
      "Iteration 106, loss = 0.37585583\n",
      "Iteration 107, loss = 0.37470184\n",
      "Iteration 108, loss = 0.37448889\n",
      "Iteration 109, loss = 0.37542788\n",
      "Iteration 110, loss = 0.37708052\n",
      "Iteration 111, loss = 0.37522643\n",
      "Iteration 112, loss = 0.37586919\n",
      "Iteration 113, loss = 0.37440365\n",
      "Iteration 114, loss = 0.37560137\n",
      "Iteration 115, loss = 0.37528900\n",
      "Iteration 116, loss = 0.37519373\n",
      "Iteration 117, loss = 0.37433835\n",
      "Iteration 118, loss = 0.37460878\n",
      "Iteration 119, loss = 0.37686024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "f1: 84.28, acc: 84.28\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler_x = MinMaxScaler()\n",
    "base_dropna_x = base_dropna.drop(columns='ChargeOff')\n",
    "base_dropna_y = base_dropna['ChargeOff']\n",
    "base_dropna_scaled_x = min_max_scaler_x.fit_transform(base_dropna_x.to_numpy())\n",
    "base_dropna_normalized_x = pd.DataFrame(base_dropna_scaled_x)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(base_dropna_normalized_x, base_dropna_y, test_size = 0.25, random_state=0)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(16,4)).fit(base_dropna_normalized_x, base_dropna_y)\n",
    "\n",
    "y_val_pred = clf.predict(x_val)\n",
    "f1 = round(f1_score(y_val, y_val_pred, average='weighted') * 100, 2)\n",
    "acc = round(accuracy_score(y_val, y_val_pred) * 100, 2)\n",
    "print(\"f1: {}, acc: {}\".format(f1, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.to_numpy().reshape((x_train.shape[0], 1, x_train.shape[1]))\n",
    "x_val = x_val.to_numpy().reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 64)                22016     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 24,129\n",
      "Trainable params: 24,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(64, input_shape=(x_train.shape[1], x_train.shape[2])),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.003),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37356 samples, validate on 12452 samples\n",
      "Epoch 1/300\n",
      "37356/37356 - 6s - loss: 0.1833 - accuracy: 0.7351 - val_loss: 0.1576 - val_accuracy: 0.7812\n",
      "Epoch 2/300\n",
      "37356/37356 - 1s - loss: 0.1501 - accuracy: 0.7936 - val_loss: 0.1505 - val_accuracy: 0.7990\n",
      "Epoch 3/300\n",
      "37356/37356 - 1s - loss: 0.1431 - accuracy: 0.8061 - val_loss: 0.1390 - val_accuracy: 0.8118\n",
      "Epoch 4/300\n",
      "37356/37356 - 1s - loss: 0.1417 - accuracy: 0.8062 - val_loss: 0.1376 - val_accuracy: 0.8183\n",
      "Epoch 5/300\n",
      "37356/37356 - 1s - loss: 0.1386 - accuracy: 0.8114 - val_loss: 0.1436 - val_accuracy: 0.7989\n",
      "Epoch 6/300\n",
      "37356/37356 - 1s - loss: 0.1377 - accuracy: 0.8103 - val_loss: 0.1349 - val_accuracy: 0.8144\n",
      "Epoch 7/300\n",
      "37356/37356 - 1s - loss: 0.1355 - accuracy: 0.8141 - val_loss: 0.1334 - val_accuracy: 0.8190\n",
      "Epoch 8/300\n",
      "37356/37356 - 1s - loss: 0.1345 - accuracy: 0.8155 - val_loss: 0.1328 - val_accuracy: 0.8193\n",
      "Epoch 9/300\n",
      "37356/37356 - 1s - loss: 0.1334 - accuracy: 0.8179 - val_loss: 0.1326 - val_accuracy: 0.8198\n",
      "Epoch 10/300\n",
      "37356/37356 - 1s - loss: 0.1329 - accuracy: 0.8176 - val_loss: 0.1308 - val_accuracy: 0.8206\n",
      "Epoch 11/300\n",
      "37356/37356 - 1s - loss: 0.1312 - accuracy: 0.8201 - val_loss: 0.1307 - val_accuracy: 0.8216\n",
      "Epoch 12/300\n",
      "37356/37356 - 1s - loss: 0.1298 - accuracy: 0.8226 - val_loss: 0.1298 - val_accuracy: 0.8219\n",
      "Epoch 13/300\n",
      "37356/37356 - 1s - loss: 0.1296 - accuracy: 0.8213 - val_loss: 0.1478 - val_accuracy: 0.7957\n",
      "Epoch 14/300\n",
      "37356/37356 - 1s - loss: 0.1286 - accuracy: 0.8228 - val_loss: 0.1277 - val_accuracy: 0.8255\n",
      "Epoch 15/300\n",
      "37356/37356 - 1s - loss: 0.1276 - accuracy: 0.8248 - val_loss: 0.1296 - val_accuracy: 0.8260\n",
      "Epoch 16/300\n",
      "37356/37356 - 1s - loss: 0.1267 - accuracy: 0.8258 - val_loss: 0.1272 - val_accuracy: 0.8268\n",
      "Epoch 17/300\n",
      "37356/37356 - 1s - loss: 0.1253 - accuracy: 0.8292 - val_loss: 0.1252 - val_accuracy: 0.8282\n",
      "Epoch 18/300\n",
      "37356/37356 - 1s - loss: 0.1255 - accuracy: 0.8290 - val_loss: 0.1243 - val_accuracy: 0.8309\n",
      "Epoch 19/300\n",
      "37356/37356 - 1s - loss: 0.1242 - accuracy: 0.8310 - val_loss: 0.1236 - val_accuracy: 0.8299\n",
      "Epoch 20/300\n",
      "37356/37356 - 1s - loss: 0.1231 - accuracy: 0.8325 - val_loss: 0.1259 - val_accuracy: 0.8273\n",
      "Epoch 21/300\n",
      "37356/37356 - 1s - loss: 0.1216 - accuracy: 0.8340 - val_loss: 0.1208 - val_accuracy: 0.8357\n",
      "Epoch 22/300\n",
      "37356/37356 - 1s - loss: 0.1207 - accuracy: 0.8358 - val_loss: 0.1213 - val_accuracy: 0.8359\n",
      "Epoch 23/300\n",
      "37356/37356 - 1s - loss: 0.1195 - accuracy: 0.8374 - val_loss: 0.1197 - val_accuracy: 0.8363\n",
      "Epoch 24/300\n",
      "37356/37356 - 1s - loss: 0.1183 - accuracy: 0.8405 - val_loss: 0.1209 - val_accuracy: 0.8383\n",
      "Epoch 25/300\n",
      "37356/37356 - 1s - loss: 0.1176 - accuracy: 0.8415 - val_loss: 0.1200 - val_accuracy: 0.8389\n",
      "Epoch 26/300\n",
      "37356/37356 - 1s - loss: 0.1170 - accuracy: 0.8414 - val_loss: 0.1207 - val_accuracy: 0.8371\n",
      "Epoch 27/300\n",
      "37356/37356 - 1s - loss: 0.1166 - accuracy: 0.8421 - val_loss: 0.1202 - val_accuracy: 0.8399\n",
      "Epoch 28/300\n",
      "37356/37356 - 1s - loss: 0.1161 - accuracy: 0.8438 - val_loss: 0.1170 - val_accuracy: 0.8451\n",
      "Epoch 29/300\n",
      "37356/37356 - 1s - loss: 0.1157 - accuracy: 0.8428 - val_loss: 0.1184 - val_accuracy: 0.8395\n",
      "Epoch 30/300\n",
      "37356/37356 - 1s - loss: 0.1147 - accuracy: 0.8456 - val_loss: 0.1173 - val_accuracy: 0.8435\n",
      "Epoch 31/300\n",
      "37356/37356 - 1s - loss: 0.1139 - accuracy: 0.8469 - val_loss: 0.1155 - val_accuracy: 0.8444\n",
      "Epoch 32/300\n",
      "37356/37356 - 1s - loss: 0.1127 - accuracy: 0.8488 - val_loss: 0.1164 - val_accuracy: 0.8434\n",
      "Epoch 33/300\n",
      "37356/37356 - 1s - loss: 0.1126 - accuracy: 0.8491 - val_loss: 0.1192 - val_accuracy: 0.8446\n",
      "Epoch 34/300\n",
      "37356/37356 - 1s - loss: 0.1122 - accuracy: 0.8490 - val_loss: 0.1132 - val_accuracy: 0.8484\n",
      "Epoch 35/300\n",
      "37356/37356 - 1s - loss: 0.1119 - accuracy: 0.8494 - val_loss: 0.1144 - val_accuracy: 0.8469\n",
      "Epoch 36/300\n",
      "37356/37356 - 1s - loss: 0.1120 - accuracy: 0.8506 - val_loss: 0.1155 - val_accuracy: 0.8440\n",
      "Epoch 37/300\n",
      "37356/37356 - 1s - loss: 0.1119 - accuracy: 0.8502 - val_loss: 0.1142 - val_accuracy: 0.8473\n",
      "Epoch 38/300\n",
      "37356/37356 - 1s - loss: 0.1110 - accuracy: 0.8513 - val_loss: 0.1157 - val_accuracy: 0.8464\n",
      "Epoch 39/300\n",
      "37356/37356 - 1s - loss: 0.1106 - accuracy: 0.8506 - val_loss: 0.1143 - val_accuracy: 0.8470\n",
      "Epoch 40/300\n",
      "37356/37356 - 1s - loss: 0.1101 - accuracy: 0.8529 - val_loss: 0.1120 - val_accuracy: 0.8482\n",
      "Epoch 41/300\n",
      "37356/37356 - 1s - loss: 0.1097 - accuracy: 0.8527 - val_loss: 0.1126 - val_accuracy: 0.8473\n",
      "Epoch 42/300\n",
      "37356/37356 - 1s - loss: 0.1088 - accuracy: 0.8548 - val_loss: 0.1170 - val_accuracy: 0.8422\n",
      "Epoch 43/300\n",
      "37356/37356 - 1s - loss: 0.1088 - accuracy: 0.8546 - val_loss: 0.1159 - val_accuracy: 0.8458\n",
      "Epoch 44/300\n",
      "37356/37356 - 1s - loss: 0.1080 - accuracy: 0.8571 - val_loss: 0.1113 - val_accuracy: 0.8510\n",
      "Epoch 45/300\n",
      "37356/37356 - 1s - loss: 0.1084 - accuracy: 0.8552 - val_loss: 0.1143 - val_accuracy: 0.8455\n",
      "Epoch 46/300\n",
      "37356/37356 - 1s - loss: 0.1076 - accuracy: 0.8570 - val_loss: 0.1129 - val_accuracy: 0.8493\n",
      "Epoch 47/300\n",
      "37356/37356 - 1s - loss: 0.1074 - accuracy: 0.8561 - val_loss: 0.1150 - val_accuracy: 0.8440\n",
      "Epoch 48/300\n",
      "37356/37356 - 1s - loss: 0.1066 - accuracy: 0.8589 - val_loss: 0.1109 - val_accuracy: 0.8509\n",
      "Epoch 49/300\n",
      "37356/37356 - 1s - loss: 0.1066 - accuracy: 0.8581 - val_loss: 0.1093 - val_accuracy: 0.8537\n",
      "Epoch 50/300\n",
      "37356/37356 - 1s - loss: 0.1063 - accuracy: 0.8582 - val_loss: 0.1157 - val_accuracy: 0.8409\n",
      "Epoch 51/300\n",
      "37356/37356 - 1s - loss: 0.1076 - accuracy: 0.8557 - val_loss: 0.1114 - val_accuracy: 0.8520\n",
      "Epoch 52/300\n",
      "37356/37356 - 1s - loss: 0.1057 - accuracy: 0.8600 - val_loss: 0.1100 - val_accuracy: 0.8513\n",
      "Epoch 53/300\n",
      "37356/37356 - 1s - loss: 0.1061 - accuracy: 0.8588 - val_loss: 0.1116 - val_accuracy: 0.8506\n",
      "Epoch 54/300\n",
      "37356/37356 - 1s - loss: 0.1052 - accuracy: 0.8602 - val_loss: 0.1094 - val_accuracy: 0.8532\n",
      "Epoch 55/300\n",
      "37356/37356 - 1s - loss: 0.1053 - accuracy: 0.8604 - val_loss: 0.1122 - val_accuracy: 0.8482\n",
      "Epoch 56/300\n",
      "37356/37356 - 1s - loss: 0.1053 - accuracy: 0.8593 - val_loss: 0.1082 - val_accuracy: 0.8542\n",
      "Epoch 57/300\n",
      "37356/37356 - 1s - loss: 0.1044 - accuracy: 0.8614 - val_loss: 0.1079 - val_accuracy: 0.8567\n",
      "Epoch 58/300\n",
      "37356/37356 - 1s - loss: 0.1039 - accuracy: 0.8613 - val_loss: 0.1122 - val_accuracy: 0.8499\n",
      "Epoch 59/300\n",
      "37356/37356 - 1s - loss: 0.1044 - accuracy: 0.8621 - val_loss: 0.1089 - val_accuracy: 0.8550\n",
      "Epoch 60/300\n",
      "37356/37356 - 1s - loss: 0.1035 - accuracy: 0.8628 - val_loss: 0.1112 - val_accuracy: 0.8486\n",
      "Epoch 61/300\n",
      "37356/37356 - 1s - loss: 0.1034 - accuracy: 0.8624 - val_loss: 0.1108 - val_accuracy: 0.8502\n",
      "Epoch 62/300\n",
      "37356/37356 - 1s - loss: 0.1031 - accuracy: 0.8623 - val_loss: 0.1131 - val_accuracy: 0.8481\n",
      "Epoch 63/300\n",
      "37356/37356 - 1s - loss: 0.1039 - accuracy: 0.8621 - val_loss: 0.1085 - val_accuracy: 0.8548\n",
      "Epoch 64/300\n",
      "37356/37356 - 1s - loss: 0.1029 - accuracy: 0.8633 - val_loss: 0.1091 - val_accuracy: 0.8538\n",
      "Epoch 65/300\n",
      "37356/37356 - 1s - loss: 0.1031 - accuracy: 0.8626 - val_loss: 0.1078 - val_accuracy: 0.8575\n",
      "Epoch 66/300\n",
      "37356/37356 - 1s - loss: 0.1024 - accuracy: 0.8643 - val_loss: 0.1082 - val_accuracy: 0.8536\n",
      "Epoch 67/300\n",
      "37356/37356 - 1s - loss: 0.1024 - accuracy: 0.8634 - val_loss: 0.1092 - val_accuracy: 0.8509\n",
      "Epoch 68/300\n",
      "37356/37356 - 1s - loss: 0.1022 - accuracy: 0.8645 - val_loss: 0.1097 - val_accuracy: 0.8517\n",
      "Epoch 69/300\n",
      "37356/37356 - 1s - loss: 0.1014 - accuracy: 0.8659 - val_loss: 0.1121 - val_accuracy: 0.8448\n",
      "Epoch 70/300\n",
      "37356/37356 - 1s - loss: 0.1016 - accuracy: 0.8658 - val_loss: 0.1145 - val_accuracy: 0.8446\n",
      "Epoch 71/300\n",
      "37356/37356 - 1s - loss: 0.1016 - accuracy: 0.8669 - val_loss: 0.1078 - val_accuracy: 0.8561\n",
      "Epoch 72/300\n",
      "37356/37356 - 1s - loss: 0.1011 - accuracy: 0.8660 - val_loss: 0.1059 - val_accuracy: 0.8555\n",
      "Epoch 73/300\n",
      "37356/37356 - 1s - loss: 0.1007 - accuracy: 0.8664 - val_loss: 0.1079 - val_accuracy: 0.8546\n",
      "Epoch 74/300\n",
      "37356/37356 - 1s - loss: 0.1014 - accuracy: 0.8660 - val_loss: 0.1092 - val_accuracy: 0.8526\n",
      "Epoch 75/300\n",
      "37356/37356 - 1s - loss: 0.1016 - accuracy: 0.8648 - val_loss: 0.1080 - val_accuracy: 0.8571\n",
      "Epoch 76/300\n",
      "37356/37356 - 1s - loss: 0.1010 - accuracy: 0.8668 - val_loss: 0.1063 - val_accuracy: 0.8587\n",
      "Epoch 77/300\n",
      "37356/37356 - 1s - loss: 0.1004 - accuracy: 0.8667 - val_loss: 0.1087 - val_accuracy: 0.8559\n",
      "Epoch 78/300\n",
      "37356/37356 - 1s - loss: 0.1010 - accuracy: 0.8661 - val_loss: 0.1061 - val_accuracy: 0.8566\n",
      "Epoch 79/300\n",
      "37356/37356 - 1s - loss: 0.1004 - accuracy: 0.8677 - val_loss: 0.1070 - val_accuracy: 0.8545\n",
      "Epoch 80/300\n",
      "37356/37356 - 1s - loss: 0.1005 - accuracy: 0.8664 - val_loss: 0.1072 - val_accuracy: 0.8545\n",
      "Epoch 81/300\n",
      "37356/37356 - 1s - loss: 0.0997 - accuracy: 0.8682 - val_loss: 0.1062 - val_accuracy: 0.8583\n",
      "Epoch 82/300\n",
      "37356/37356 - 1s - loss: 0.0998 - accuracy: 0.8676 - val_loss: 0.1066 - val_accuracy: 0.8563\n",
      "Epoch 83/300\n",
      "37356/37356 - 1s - loss: 0.0996 - accuracy: 0.8685 - val_loss: 0.1131 - val_accuracy: 0.8538\n",
      "Epoch 84/300\n",
      "37356/37356 - 1s - loss: 0.1001 - accuracy: 0.8677 - val_loss: 0.1072 - val_accuracy: 0.8576\n",
      "Epoch 85/300\n",
      "37356/37356 - 1s - loss: 0.0996 - accuracy: 0.8673 - val_loss: 0.1090 - val_accuracy: 0.8546\n",
      "Epoch 86/300\n",
      "37356/37356 - 1s - loss: 0.0999 - accuracy: 0.8683 - val_loss: 0.1075 - val_accuracy: 0.8547\n",
      "Epoch 87/300\n",
      "37356/37356 - 1s - loss: 0.0996 - accuracy: 0.8673 - val_loss: 0.1046 - val_accuracy: 0.8580\n",
      "Epoch 88/300\n",
      "37356/37356 - 1s - loss: 0.0989 - accuracy: 0.8698 - val_loss: 0.1086 - val_accuracy: 0.8534\n",
      "Epoch 89/300\n",
      "37356/37356 - 1s - loss: 0.0994 - accuracy: 0.8679 - val_loss: 0.1049 - val_accuracy: 0.8607\n",
      "Epoch 90/300\n",
      "37356/37356 - 1s - loss: 0.0986 - accuracy: 0.8691 - val_loss: 0.1090 - val_accuracy: 0.8543\n",
      "Epoch 91/300\n",
      "37356/37356 - 1s - loss: 0.0989 - accuracy: 0.8687 - val_loss: 0.1085 - val_accuracy: 0.8571\n",
      "Epoch 92/300\n",
      "37356/37356 - 1s - loss: 0.0989 - accuracy: 0.8691 - val_loss: 0.1075 - val_accuracy: 0.8546\n",
      "Epoch 93/300\n",
      "37356/37356 - 1s - loss: 0.0993 - accuracy: 0.8679 - val_loss: 0.1052 - val_accuracy: 0.8603\n",
      "Epoch 94/300\n",
      "37356/37356 - 1s - loss: 0.0989 - accuracy: 0.8679 - val_loss: 0.1047 - val_accuracy: 0.8603\n",
      "Epoch 95/300\n",
      "37356/37356 - 1s - loss: 0.0982 - accuracy: 0.8701 - val_loss: 0.1052 - val_accuracy: 0.8599\n",
      "Epoch 96/300\n",
      "37356/37356 - 1s - loss: 0.0977 - accuracy: 0.8700 - val_loss: 0.1043 - val_accuracy: 0.8609\n",
      "Epoch 97/300\n",
      "37356/37356 - 1s - loss: 0.0983 - accuracy: 0.8712 - val_loss: 0.1086 - val_accuracy: 0.8568\n",
      "Epoch 98/300\n",
      "37356/37356 - 1s - loss: 0.0980 - accuracy: 0.8707 - val_loss: 0.1050 - val_accuracy: 0.8611\n",
      "Epoch 99/300\n",
      "37356/37356 - 1s - loss: 0.0978 - accuracy: 0.8713 - val_loss: 0.1044 - val_accuracy: 0.8586\n",
      "Epoch 100/300\n",
      "37356/37356 - 1s - loss: 0.0978 - accuracy: 0.8711 - val_loss: 0.1069 - val_accuracy: 0.8583\n",
      "Epoch 101/300\n",
      "37356/37356 - 1s - loss: 0.0975 - accuracy: 0.8709 - val_loss: 0.1066 - val_accuracy: 0.8563\n",
      "Epoch 102/300\n",
      "37356/37356 - 1s - loss: 0.0976 - accuracy: 0.8713 - val_loss: 0.1046 - val_accuracy: 0.8594\n",
      "Epoch 103/300\n",
      "37356/37356 - 1s - loss: 0.0967 - accuracy: 0.8729 - val_loss: 0.1052 - val_accuracy: 0.8589\n",
      "Epoch 104/300\n",
      "37356/37356 - 1s - loss: 0.0970 - accuracy: 0.8729 - val_loss: 0.1030 - val_accuracy: 0.8607\n",
      "Epoch 105/300\n",
      "37356/37356 - 1s - loss: 0.0970 - accuracy: 0.8720 - val_loss: 0.1063 - val_accuracy: 0.8562\n",
      "Epoch 106/300\n",
      "37356/37356 - 1s - loss: 0.0969 - accuracy: 0.8730 - val_loss: 0.1057 - val_accuracy: 0.8593\n",
      "Epoch 107/300\n",
      "37356/37356 - 1s - loss: 0.0970 - accuracy: 0.8723 - val_loss: 0.1052 - val_accuracy: 0.8587\n",
      "Epoch 108/300\n",
      "37356/37356 - 1s - loss: 0.0960 - accuracy: 0.8742 - val_loss: 0.1044 - val_accuracy: 0.8626\n",
      "Epoch 109/300\n",
      "37356/37356 - 1s - loss: 0.0971 - accuracy: 0.8716 - val_loss: 0.1041 - val_accuracy: 0.8603\n",
      "Epoch 110/300\n",
      "37356/37356 - 1s - loss: 0.0972 - accuracy: 0.8713 - val_loss: 0.1059 - val_accuracy: 0.8563\n",
      "Epoch 111/300\n",
      "37356/37356 - 1s - loss: 0.0956 - accuracy: 0.8736 - val_loss: 0.1073 - val_accuracy: 0.8544\n",
      "Epoch 112/300\n",
      "37356/37356 - 1s - loss: 0.0961 - accuracy: 0.8744 - val_loss: 0.1073 - val_accuracy: 0.8558\n",
      "Epoch 113/300\n",
      "37356/37356 - 1s - loss: 0.0959 - accuracy: 0.8740 - val_loss: 0.1053 - val_accuracy: 0.8622\n",
      "Epoch 114/300\n",
      "37356/37356 - 1s - loss: 0.0959 - accuracy: 0.8742 - val_loss: 0.1034 - val_accuracy: 0.8628\n",
      "Epoch 115/300\n",
      "37356/37356 - 1s - loss: 0.0961 - accuracy: 0.8724 - val_loss: 0.1025 - val_accuracy: 0.8636\n",
      "Epoch 116/300\n",
      "37356/37356 - 1s - loss: 0.0954 - accuracy: 0.8752 - val_loss: 0.1071 - val_accuracy: 0.8562\n",
      "Epoch 117/300\n",
      "37356/37356 - 1s - loss: 0.0953 - accuracy: 0.8745 - val_loss: 0.1047 - val_accuracy: 0.8575\n",
      "Epoch 118/300\n",
      "37356/37356 - 1s - loss: 0.0949 - accuracy: 0.8758 - val_loss: 0.1038 - val_accuracy: 0.8607\n",
      "Epoch 119/300\n",
      "37356/37356 - 1s - loss: 0.0951 - accuracy: 0.8740 - val_loss: 0.1054 - val_accuracy: 0.8583\n",
      "Epoch 120/300\n",
      "37356/37356 - 1s - loss: 0.0958 - accuracy: 0.8731 - val_loss: 0.1053 - val_accuracy: 0.8591\n",
      "Epoch 121/300\n",
      "37356/37356 - 1s - loss: 0.0953 - accuracy: 0.8754 - val_loss: 0.1029 - val_accuracy: 0.8632\n",
      "Epoch 122/300\n",
      "37356/37356 - 1s - loss: 0.0958 - accuracy: 0.8726 - val_loss: 0.1088 - val_accuracy: 0.8537\n",
      "Epoch 123/300\n",
      "37356/37356 - 1s - loss: 0.0948 - accuracy: 0.8747 - val_loss: 0.1037 - val_accuracy: 0.8604\n",
      "Epoch 124/300\n",
      "37356/37356 - 1s - loss: 0.0951 - accuracy: 0.8749 - val_loss: 0.1032 - val_accuracy: 0.8626\n",
      "Epoch 125/300\n",
      "37356/37356 - 1s - loss: 0.0947 - accuracy: 0.8752 - val_loss: 0.1058 - val_accuracy: 0.8608\n",
      "Epoch 126/300\n",
      "37356/37356 - 1s - loss: 0.0941 - accuracy: 0.8760 - val_loss: 0.1035 - val_accuracy: 0.8606\n",
      "Epoch 127/300\n",
      "37356/37356 - 1s - loss: 0.0945 - accuracy: 0.8753 - val_loss: 0.1047 - val_accuracy: 0.8607\n",
      "Epoch 128/300\n",
      "37356/37356 - 1s - loss: 0.0946 - accuracy: 0.8757 - val_loss: 0.1054 - val_accuracy: 0.8592\n",
      "Epoch 129/300\n",
      "37356/37356 - 1s - loss: 0.0943 - accuracy: 0.8761 - val_loss: 0.1054 - val_accuracy: 0.8598\n",
      "Epoch 130/300\n",
      "37356/37356 - 1s - loss: 0.0947 - accuracy: 0.8754 - val_loss: 0.1045 - val_accuracy: 0.8622\n",
      "Epoch 131/300\n",
      "37356/37356 - 1s - loss: 0.0948 - accuracy: 0.8755 - val_loss: 0.1087 - val_accuracy: 0.8589\n",
      "Epoch 132/300\n",
      "37356/37356 - 1s - loss: 0.0941 - accuracy: 0.8771 - val_loss: 0.1044 - val_accuracy: 0.8608\n",
      "Epoch 133/300\n",
      "37356/37356 - 1s - loss: 0.0932 - accuracy: 0.8771 - val_loss: 0.1042 - val_accuracy: 0.8612\n",
      "Epoch 134/300\n",
      "37356/37356 - 1s - loss: 0.0935 - accuracy: 0.8772 - val_loss: 0.1047 - val_accuracy: 0.8595\n",
      "Epoch 135/300\n",
      "37356/37356 - 1s - loss: 0.0934 - accuracy: 0.8757 - val_loss: 0.1032 - val_accuracy: 0.8602\n",
      "Epoch 136/300\n",
      "37356/37356 - 1s - loss: 0.0941 - accuracy: 0.8751 - val_loss: 0.1087 - val_accuracy: 0.8540\n",
      "Epoch 137/300\n",
      "37356/37356 - 1s - loss: 0.0941 - accuracy: 0.8755 - val_loss: 0.1036 - val_accuracy: 0.8619\n",
      "Epoch 138/300\n",
      "37356/37356 - 1s - loss: 0.0942 - accuracy: 0.8768 - val_loss: 0.1032 - val_accuracy: 0.8610\n",
      "Epoch 139/300\n",
      "37356/37356 - 1s - loss: 0.0946 - accuracy: 0.8743 - val_loss: 0.1030 - val_accuracy: 0.8615\n",
      "Epoch 140/300\n",
      "37356/37356 - 1s - loss: 0.0936 - accuracy: 0.8772 - val_loss: 0.1034 - val_accuracy: 0.8628\n",
      "Epoch 141/300\n",
      "37356/37356 - 1s - loss: 0.0926 - accuracy: 0.8780 - val_loss: 0.1050 - val_accuracy: 0.8622\n",
      "Epoch 142/300\n",
      "37356/37356 - 1s - loss: 0.0926 - accuracy: 0.8779 - val_loss: 0.1044 - val_accuracy: 0.8599\n",
      "Epoch 143/300\n",
      "37356/37356 - 1s - loss: 0.0924 - accuracy: 0.8776 - val_loss: 0.1030 - val_accuracy: 0.8657\n",
      "Epoch 144/300\n",
      "37356/37356 - 1s - loss: 0.0939 - accuracy: 0.8765 - val_loss: 0.1052 - val_accuracy: 0.8590\n",
      "Epoch 145/300\n",
      "37356/37356 - 1s - loss: 0.0945 - accuracy: 0.8749 - val_loss: 0.1038 - val_accuracy: 0.8611\n",
      "Epoch 146/300\n",
      "37356/37356 - 1s - loss: 0.0931 - accuracy: 0.8776 - val_loss: 0.1055 - val_accuracy: 0.8603\n",
      "Epoch 147/300\n",
      "37356/37356 - 1s - loss: 0.0924 - accuracy: 0.8787 - val_loss: 0.1045 - val_accuracy: 0.8597\n",
      "Epoch 148/300\n",
      "37356/37356 - 1s - loss: 0.0927 - accuracy: 0.8787 - val_loss: 0.1043 - val_accuracy: 0.8614\n",
      "Epoch 149/300\n",
      "37356/37356 - 1s - loss: 0.0931 - accuracy: 0.8783 - val_loss: 0.1048 - val_accuracy: 0.8588\n",
      "Epoch 150/300\n",
      "37356/37356 - 1s - loss: 0.0931 - accuracy: 0.8782 - val_loss: 0.1033 - val_accuracy: 0.8623\n",
      "Epoch 151/300\n",
      "37356/37356 - 1s - loss: 0.0925 - accuracy: 0.8789 - val_loss: 0.1047 - val_accuracy: 0.8595\n",
      "Epoch 152/300\n",
      "37356/37356 - 1s - loss: 0.0931 - accuracy: 0.8786 - val_loss: 0.1031 - val_accuracy: 0.8633\n",
      "Epoch 153/300\n",
      "37356/37356 - 1s - loss: 0.0919 - accuracy: 0.8791 - val_loss: 0.1043 - val_accuracy: 0.8623\n",
      "Epoch 154/300\n",
      "37356/37356 - 1s - loss: 0.0924 - accuracy: 0.8790 - val_loss: 0.1064 - val_accuracy: 0.8597\n",
      "Epoch 155/300\n",
      "37356/37356 - 1s - loss: 0.0916 - accuracy: 0.8802 - val_loss: 0.1039 - val_accuracy: 0.8614\n",
      "Epoch 156/300\n",
      "37356/37356 - 1s - loss: 0.0918 - accuracy: 0.8794 - val_loss: 0.1040 - val_accuracy: 0.8612\n",
      "Epoch 157/300\n",
      "37356/37356 - 1s - loss: 0.0914 - accuracy: 0.8814 - val_loss: 0.1061 - val_accuracy: 0.8599\n",
      "Epoch 158/300\n",
      "37356/37356 - 1s - loss: 0.0920 - accuracy: 0.8794 - val_loss: 0.1036 - val_accuracy: 0.8605\n",
      "Epoch 159/300\n",
      "37356/37356 - 1s - loss: 0.0917 - accuracy: 0.8795 - val_loss: 0.1047 - val_accuracy: 0.8599\n",
      "Epoch 160/300\n",
      "37356/37356 - 1s - loss: 0.0916 - accuracy: 0.8791 - val_loss: 0.1043 - val_accuracy: 0.8612\n",
      "Epoch 161/300\n",
      "37356/37356 - 1s - loss: 0.0915 - accuracy: 0.8789 - val_loss: 0.1110 - val_accuracy: 0.8496\n",
      "Epoch 162/300\n",
      "37356/37356 - 1s - loss: 0.0916 - accuracy: 0.8815 - val_loss: 0.1047 - val_accuracy: 0.8617\n",
      "Epoch 163/300\n",
      "37356/37356 - 1s - loss: 0.0914 - accuracy: 0.8798 - val_loss: 0.1034 - val_accuracy: 0.8611\n",
      "Epoch 164/300\n",
      "37356/37356 - 1s - loss: 0.0910 - accuracy: 0.8820 - val_loss: 0.1050 - val_accuracy: 0.8602\n",
      "Epoch 165/300\n",
      "37356/37356 - 1s - loss: 0.0910 - accuracy: 0.8809 - val_loss: 0.1059 - val_accuracy: 0.8634\n",
      "Epoch 166/300\n",
      "37356/37356 - 1s - loss: 0.0910 - accuracy: 0.8813 - val_loss: 0.1037 - val_accuracy: 0.8615\n",
      "Epoch 167/300\n",
      "37356/37356 - 1s - loss: 0.0910 - accuracy: 0.8807 - val_loss: 0.1041 - val_accuracy: 0.8599\n",
      "Epoch 168/300\n",
      "37356/37356 - 1s - loss: 0.0901 - accuracy: 0.8805 - val_loss: 0.1065 - val_accuracy: 0.8590\n",
      "Epoch 169/300\n",
      "37356/37356 - 1s - loss: 0.0901 - accuracy: 0.8821 - val_loss: 0.1055 - val_accuracy: 0.8585\n",
      "Epoch 170/300\n",
      "37356/37356 - 1s - loss: 0.0905 - accuracy: 0.8823 - val_loss: 0.1028 - val_accuracy: 0.8615\n",
      "Epoch 171/300\n",
      "37356/37356 - 1s - loss: 0.0906 - accuracy: 0.8815 - val_loss: 0.1063 - val_accuracy: 0.8593\n",
      "Epoch 172/300\n",
      "37356/37356 - 1s - loss: 0.0903 - accuracy: 0.8820 - val_loss: 0.1034 - val_accuracy: 0.8619\n",
      "Epoch 173/300\n",
      "37356/37356 - 1s - loss: 0.0904 - accuracy: 0.8812 - val_loss: 0.1053 - val_accuracy: 0.8611\n",
      "Epoch 174/300\n",
      "37356/37356 - 1s - loss: 0.0903 - accuracy: 0.8813 - val_loss: 0.1034 - val_accuracy: 0.8624\n",
      "Epoch 175/300\n",
      "37356/37356 - 1s - loss: 0.0904 - accuracy: 0.8810 - val_loss: 0.1044 - val_accuracy: 0.8591\n",
      "Epoch 176/300\n",
      "37356/37356 - 1s - loss: 0.0903 - accuracy: 0.8818 - val_loss: 0.1072 - val_accuracy: 0.8582\n",
      "Epoch 177/300\n",
      "37356/37356 - 1s - loss: 0.0896 - accuracy: 0.8824 - val_loss: 0.1026 - val_accuracy: 0.8648\n",
      "Epoch 178/300\n",
      "37356/37356 - 1s - loss: 0.0905 - accuracy: 0.8806 - val_loss: 0.1035 - val_accuracy: 0.8644\n",
      "Epoch 179/300\n",
      "37356/37356 - 1s - loss: 0.0898 - accuracy: 0.8829 - val_loss: 0.1047 - val_accuracy: 0.8636\n",
      "Epoch 180/300\n",
      "37356/37356 - 1s - loss: 0.0903 - accuracy: 0.8813 - val_loss: 0.1059 - val_accuracy: 0.8597\n",
      "Epoch 181/300\n",
      "37356/37356 - 1s - loss: 0.0902 - accuracy: 0.8827 - val_loss: 0.1045 - val_accuracy: 0.8619\n",
      "Epoch 182/300\n",
      "37356/37356 - 1s - loss: 0.0898 - accuracy: 0.8813 - val_loss: 0.1028 - val_accuracy: 0.8644\n",
      "Epoch 183/300\n",
      "37356/37356 - 1s - loss: 0.0894 - accuracy: 0.8828 - val_loss: 0.1056 - val_accuracy: 0.8601\n",
      "Epoch 184/300\n",
      "37356/37356 - 1s - loss: 0.0901 - accuracy: 0.8824 - val_loss: 0.1035 - val_accuracy: 0.8634\n",
      "Epoch 185/300\n",
      "37356/37356 - 1s - loss: 0.0899 - accuracy: 0.8819 - val_loss: 0.1081 - val_accuracy: 0.8561\n",
      "Epoch 186/300\n",
      "37356/37356 - 1s - loss: 0.0896 - accuracy: 0.8833 - val_loss: 0.1035 - val_accuracy: 0.8654\n",
      "Epoch 187/300\n",
      "37356/37356 - 1s - loss: 0.0892 - accuracy: 0.8840 - val_loss: 0.1052 - val_accuracy: 0.8619\n",
      "Epoch 188/300\n",
      "37356/37356 - 1s - loss: 0.0887 - accuracy: 0.8844 - val_loss: 0.1032 - val_accuracy: 0.8635\n",
      "Epoch 189/300\n",
      "37356/37356 - 1s - loss: 0.0898 - accuracy: 0.8822 - val_loss: 0.1043 - val_accuracy: 0.8615\n",
      "Epoch 190/300\n",
      "37356/37356 - 1s - loss: 0.0893 - accuracy: 0.8827 - val_loss: 0.1045 - val_accuracy: 0.8603\n",
      "Epoch 191/300\n",
      "37356/37356 - 1s - loss: 0.0891 - accuracy: 0.8834 - val_loss: 0.1030 - val_accuracy: 0.8640\n",
      "Epoch 192/300\n",
      "37356/37356 - 1s - loss: 0.0889 - accuracy: 0.8845 - val_loss: 0.1060 - val_accuracy: 0.8588\n",
      "Epoch 193/300\n",
      "37356/37356 - 1s - loss: 0.0889 - accuracy: 0.8839 - val_loss: 0.1036 - val_accuracy: 0.8627\n",
      "Epoch 194/300\n",
      "37356/37356 - 1s - loss: 0.0889 - accuracy: 0.8835 - val_loss: 0.1082 - val_accuracy: 0.8571\n",
      "Epoch 195/300\n",
      "37356/37356 - 1s - loss: 0.0886 - accuracy: 0.8839 - val_loss: 0.1044 - val_accuracy: 0.8612\n",
      "Epoch 196/300\n",
      "37356/37356 - 1s - loss: 0.0888 - accuracy: 0.8834 - val_loss: 0.1061 - val_accuracy: 0.8554\n",
      "Epoch 197/300\n",
      "37356/37356 - 1s - loss: 0.0894 - accuracy: 0.8830 - val_loss: 0.1045 - val_accuracy: 0.8613\n",
      "Epoch 198/300\n",
      "37356/37356 - 1s - loss: 0.0875 - accuracy: 0.8866 - val_loss: 0.1063 - val_accuracy: 0.8607\n",
      "Epoch 199/300\n",
      "37356/37356 - 1s - loss: 0.0883 - accuracy: 0.8827 - val_loss: 0.1044 - val_accuracy: 0.8628\n",
      "Epoch 200/300\n",
      "37356/37356 - 1s - loss: 0.0894 - accuracy: 0.8828 - val_loss: 0.1061 - val_accuracy: 0.8558\n",
      "Epoch 201/300\n",
      "37356/37356 - 1s - loss: 0.0887 - accuracy: 0.8840 - val_loss: 0.1045 - val_accuracy: 0.8624\n",
      "Epoch 202/300\n",
      "37356/37356 - 1s - loss: 0.0884 - accuracy: 0.8847 - val_loss: 0.1069 - val_accuracy: 0.8607\n",
      "Epoch 203/300\n",
      "37356/37356 - 1s - loss: 0.0881 - accuracy: 0.8856 - val_loss: 0.1048 - val_accuracy: 0.8639\n",
      "Epoch 204/300\n",
      "37356/37356 - 1s - loss: 0.0880 - accuracy: 0.8856 - val_loss: 0.1032 - val_accuracy: 0.8630\n",
      "Epoch 205/300\n",
      "37356/37356 - 1s - loss: 0.0879 - accuracy: 0.8859 - val_loss: 0.1053 - val_accuracy: 0.8599\n",
      "Epoch 206/300\n",
      "37356/37356 - 1s - loss: 0.0880 - accuracy: 0.8847 - val_loss: 0.1050 - val_accuracy: 0.8573\n",
      "Epoch 207/300\n",
      "37356/37356 - 1s - loss: 0.0876 - accuracy: 0.8856 - val_loss: 0.1075 - val_accuracy: 0.8572\n",
      "Epoch 208/300\n",
      "37356/37356 - 1s - loss: 0.0876 - accuracy: 0.8867 - val_loss: 0.1073 - val_accuracy: 0.8578\n",
      "Epoch 209/300\n",
      "37356/37356 - 1s - loss: 0.0877 - accuracy: 0.8856 - val_loss: 0.1050 - val_accuracy: 0.8626\n",
      "Epoch 210/300\n",
      "37356/37356 - 1s - loss: 0.0877 - accuracy: 0.8865 - val_loss: 0.1023 - val_accuracy: 0.8641\n",
      "Epoch 211/300\n",
      "37356/37356 - 1s - loss: 0.0874 - accuracy: 0.8854 - val_loss: 0.1052 - val_accuracy: 0.8607\n",
      "Epoch 212/300\n",
      "37356/37356 - 1s - loss: 0.0868 - accuracy: 0.8862 - val_loss: 0.1047 - val_accuracy: 0.8628\n",
      "Epoch 213/300\n",
      "37356/37356 - 1s - loss: 0.0870 - accuracy: 0.8861 - val_loss: 0.1038 - val_accuracy: 0.8637\n",
      "Epoch 214/300\n",
      "37356/37356 - 1s - loss: 0.0874 - accuracy: 0.8863 - val_loss: 0.1044 - val_accuracy: 0.8623\n",
      "Epoch 215/300\n",
      "37356/37356 - 1s - loss: 0.0867 - accuracy: 0.8873 - val_loss: 0.1054 - val_accuracy: 0.8634\n",
      "Epoch 216/300\n",
      "37356/37356 - 1s - loss: 0.0870 - accuracy: 0.8874 - val_loss: 0.1038 - val_accuracy: 0.8607\n",
      "Epoch 217/300\n",
      "37356/37356 - 1s - loss: 0.0872 - accuracy: 0.8866 - val_loss: 0.1045 - val_accuracy: 0.8633\n",
      "Epoch 218/300\n",
      "37356/37356 - 1s - loss: 0.0866 - accuracy: 0.8868 - val_loss: 0.1041 - val_accuracy: 0.8595\n",
      "Epoch 219/300\n",
      "37356/37356 - 1s - loss: 0.0875 - accuracy: 0.8849 - val_loss: 0.1061 - val_accuracy: 0.8595\n",
      "Epoch 220/300\n",
      "37356/37356 - 1s - loss: 0.0865 - accuracy: 0.8867 - val_loss: 0.1049 - val_accuracy: 0.8626\n",
      "Epoch 221/300\n",
      "37356/37356 - 1s - loss: 0.0871 - accuracy: 0.8863 - val_loss: 0.1050 - val_accuracy: 0.8614\n",
      "Epoch 222/300\n",
      "37356/37356 - 1s - loss: 0.0871 - accuracy: 0.8868 - val_loss: 0.1104 - val_accuracy: 0.8541\n",
      "Epoch 223/300\n",
      "37356/37356 - 1s - loss: 0.0857 - accuracy: 0.8881 - val_loss: 0.1043 - val_accuracy: 0.8613\n",
      "Epoch 224/300\n",
      "37356/37356 - 1s - loss: 0.0870 - accuracy: 0.8867 - val_loss: 0.1084 - val_accuracy: 0.8576\n",
      "Epoch 225/300\n",
      "37356/37356 - 1s - loss: 0.0866 - accuracy: 0.8873 - val_loss: 0.1052 - val_accuracy: 0.8621\n",
      "Epoch 226/300\n",
      "37356/37356 - 1s - loss: 0.0867 - accuracy: 0.8878 - val_loss: 0.1042 - val_accuracy: 0.8604\n",
      "Epoch 227/300\n",
      "37356/37356 - 1s - loss: 0.0861 - accuracy: 0.8882 - val_loss: 0.1057 - val_accuracy: 0.8624\n",
      "Epoch 228/300\n",
      "37356/37356 - 1s - loss: 0.0861 - accuracy: 0.8882 - val_loss: 0.1032 - val_accuracy: 0.8629\n",
      "Epoch 229/300\n",
      "37356/37356 - 1s - loss: 0.0865 - accuracy: 0.8882 - val_loss: 0.1111 - val_accuracy: 0.8508\n",
      "Epoch 230/300\n",
      "37356/37356 - 1s - loss: 0.0862 - accuracy: 0.8877 - val_loss: 0.1064 - val_accuracy: 0.8584\n",
      "Epoch 231/300\n",
      "37356/37356 - 1s - loss: 0.0856 - accuracy: 0.8885 - val_loss: 0.1075 - val_accuracy: 0.8561\n",
      "Epoch 232/300\n",
      "37356/37356 - 1s - loss: 0.0862 - accuracy: 0.8884 - val_loss: 0.1055 - val_accuracy: 0.8606\n",
      "Epoch 233/300\n",
      "37356/37356 - 1s - loss: 0.0854 - accuracy: 0.8892 - val_loss: 0.1046 - val_accuracy: 0.8639\n",
      "Epoch 234/300\n",
      "37356/37356 - 1s - loss: 0.0859 - accuracy: 0.8889 - val_loss: 0.1049 - val_accuracy: 0.8619\n",
      "Epoch 235/300\n",
      "37356/37356 - 1s - loss: 0.0856 - accuracy: 0.8887 - val_loss: 0.1043 - val_accuracy: 0.8632\n",
      "Epoch 236/300\n",
      "37356/37356 - 1s - loss: 0.0859 - accuracy: 0.8881 - val_loss: 0.1040 - val_accuracy: 0.8631\n",
      "Epoch 237/300\n",
      "37356/37356 - 1s - loss: 0.0855 - accuracy: 0.8892 - val_loss: 0.1081 - val_accuracy: 0.8560\n",
      "Epoch 238/300\n",
      "37356/37356 - 1s - loss: 0.0853 - accuracy: 0.8887 - val_loss: 0.1075 - val_accuracy: 0.8562\n",
      "Epoch 239/300\n",
      "37356/37356 - 1s - loss: 0.0861 - accuracy: 0.8878 - val_loss: 0.1029 - val_accuracy: 0.8657\n",
      "Epoch 240/300\n",
      "37356/37356 - 1s - loss: 0.0853 - accuracy: 0.8894 - val_loss: 0.1041 - val_accuracy: 0.8642\n",
      "Epoch 241/300\n",
      "37356/37356 - 1s - loss: 0.0851 - accuracy: 0.8891 - val_loss: 0.1117 - val_accuracy: 0.8500\n",
      "Epoch 242/300\n",
      "37356/37356 - 1s - loss: 0.0850 - accuracy: 0.8903 - val_loss: 0.1057 - val_accuracy: 0.8589\n",
      "Epoch 243/300\n",
      "37356/37356 - 1s - loss: 0.0853 - accuracy: 0.8891 - val_loss: 0.1046 - val_accuracy: 0.8603\n",
      "Epoch 244/300\n",
      "37356/37356 - 1s - loss: 0.0852 - accuracy: 0.8882 - val_loss: 0.1031 - val_accuracy: 0.8638\n",
      "Epoch 245/300\n",
      "37356/37356 - 1s - loss: 0.0851 - accuracy: 0.8900 - val_loss: 0.1048 - val_accuracy: 0.8612\n",
      "Epoch 246/300\n",
      "37356/37356 - 1s - loss: 0.0849 - accuracy: 0.8902 - val_loss: 0.1059 - val_accuracy: 0.8607\n",
      "Epoch 247/300\n",
      "37356/37356 - 1s - loss: 0.0850 - accuracy: 0.8898 - val_loss: 0.1063 - val_accuracy: 0.8604\n",
      "Epoch 248/300\n",
      "37356/37356 - 1s - loss: 0.0850 - accuracy: 0.8902 - val_loss: 0.1059 - val_accuracy: 0.8615\n",
      "Epoch 249/300\n",
      "37356/37356 - 1s - loss: 0.0842 - accuracy: 0.8913 - val_loss: 0.1046 - val_accuracy: 0.8631\n",
      "Epoch 250/300\n",
      "37356/37356 - 1s - loss: 0.0844 - accuracy: 0.8901 - val_loss: 0.1071 - val_accuracy: 0.8591\n",
      "Epoch 251/300\n",
      "37356/37356 - 1s - loss: 0.0846 - accuracy: 0.8894 - val_loss: 0.1077 - val_accuracy: 0.8564\n",
      "Epoch 252/300\n",
      "37356/37356 - 1s - loss: 0.0850 - accuracy: 0.8898 - val_loss: 0.1050 - val_accuracy: 0.8627\n",
      "Epoch 253/300\n",
      "37356/37356 - 1s - loss: 0.0851 - accuracy: 0.8895 - val_loss: 0.1058 - val_accuracy: 0.8603\n",
      "Epoch 254/300\n",
      "37356/37356 - 1s - loss: 0.0847 - accuracy: 0.8908 - val_loss: 0.1037 - val_accuracy: 0.8634\n",
      "Epoch 255/300\n",
      "37356/37356 - 1s - loss: 0.0841 - accuracy: 0.8916 - val_loss: 0.1058 - val_accuracy: 0.8586\n",
      "Epoch 256/300\n",
      "37356/37356 - 1s - loss: 0.0842 - accuracy: 0.8910 - val_loss: 0.1054 - val_accuracy: 0.8631\n",
      "Epoch 257/300\n",
      "37356/37356 - 1s - loss: 0.0836 - accuracy: 0.8918 - val_loss: 0.1044 - val_accuracy: 0.8631\n",
      "Epoch 258/300\n",
      "37356/37356 - 1s - loss: 0.0843 - accuracy: 0.8905 - val_loss: 0.1035 - val_accuracy: 0.8659\n",
      "Epoch 259/300\n",
      "37356/37356 - 1s - loss: 0.0839 - accuracy: 0.8919 - val_loss: 0.1045 - val_accuracy: 0.8627\n",
      "Epoch 260/300\n",
      "37356/37356 - 1s - loss: 0.0844 - accuracy: 0.8894 - val_loss: 0.1055 - val_accuracy: 0.8615\n",
      "Epoch 261/300\n",
      "37356/37356 - 1s - loss: 0.0844 - accuracy: 0.8902 - val_loss: 0.1039 - val_accuracy: 0.8631\n",
      "Epoch 262/300\n",
      "37356/37356 - 1s - loss: 0.0841 - accuracy: 0.8908 - val_loss: 0.1074 - val_accuracy: 0.8607\n",
      "Epoch 263/300\n",
      "37356/37356 - 1s - loss: 0.0837 - accuracy: 0.8914 - val_loss: 0.1037 - val_accuracy: 0.8641\n",
      "Epoch 264/300\n",
      "37356/37356 - 1s - loss: 0.0835 - accuracy: 0.8917 - val_loss: 0.1096 - val_accuracy: 0.8530\n",
      "Epoch 265/300\n",
      "37356/37356 - 1s - loss: 0.0844 - accuracy: 0.8898 - val_loss: 0.1051 - val_accuracy: 0.8593\n",
      "Epoch 266/300\n",
      "37356/37356 - 1s - loss: 0.0839 - accuracy: 0.8923 - val_loss: 0.1040 - val_accuracy: 0.8644\n",
      "Epoch 267/300\n",
      "37356/37356 - 1s - loss: 0.0837 - accuracy: 0.8917 - val_loss: 0.1096 - val_accuracy: 0.8554\n",
      "Epoch 268/300\n",
      "37356/37356 - 1s - loss: 0.0834 - accuracy: 0.8913 - val_loss: 0.1046 - val_accuracy: 0.8605\n",
      "Epoch 269/300\n",
      "37356/37356 - 1s - loss: 0.0833 - accuracy: 0.8934 - val_loss: 0.1055 - val_accuracy: 0.8604\n",
      "Epoch 270/300\n",
      "37356/37356 - 1s - loss: 0.0829 - accuracy: 0.8923 - val_loss: 0.1063 - val_accuracy: 0.8586\n",
      "Epoch 271/300\n",
      "37356/37356 - 1s - loss: 0.0829 - accuracy: 0.8926 - val_loss: 0.1048 - val_accuracy: 0.8582\n",
      "Epoch 272/300\n",
      "37356/37356 - 1s - loss: 0.0822 - accuracy: 0.8936 - val_loss: 0.1034 - val_accuracy: 0.8636\n",
      "Epoch 273/300\n",
      "37356/37356 - 1s - loss: 0.0826 - accuracy: 0.8923 - val_loss: 0.1059 - val_accuracy: 0.8606\n",
      "Epoch 274/300\n",
      "37356/37356 - 1s - loss: 0.0831 - accuracy: 0.8928 - val_loss: 0.1067 - val_accuracy: 0.8597\n",
      "Epoch 275/300\n",
      "37356/37356 - 1s - loss: 0.0830 - accuracy: 0.8927 - val_loss: 0.1037 - val_accuracy: 0.8624\n",
      "Epoch 276/300\n",
      "37356/37356 - 1s - loss: 0.0832 - accuracy: 0.8931 - val_loss: 0.1051 - val_accuracy: 0.8624\n",
      "Epoch 277/300\n",
      "37356/37356 - 1s - loss: 0.0832 - accuracy: 0.8914 - val_loss: 0.1056 - val_accuracy: 0.8595\n",
      "Epoch 278/300\n",
      "37356/37356 - 1s - loss: 0.0824 - accuracy: 0.8939 - val_loss: 0.1055 - val_accuracy: 0.8591\n",
      "Epoch 279/300\n",
      "37356/37356 - 1s - loss: 0.0832 - accuracy: 0.8921 - val_loss: 0.1097 - val_accuracy: 0.8550\n",
      "Epoch 280/300\n",
      "37356/37356 - 1s - loss: 0.0827 - accuracy: 0.8938 - val_loss: 0.1093 - val_accuracy: 0.8537\n",
      "Epoch 281/300\n",
      "37356/37356 - 1s - loss: 0.0828 - accuracy: 0.8925 - val_loss: 0.1049 - val_accuracy: 0.8605\n",
      "Epoch 282/300\n",
      "37356/37356 - 1s - loss: 0.0824 - accuracy: 0.8938 - val_loss: 0.1093 - val_accuracy: 0.8537\n",
      "Epoch 283/300\n",
      "37356/37356 - 1s - loss: 0.0827 - accuracy: 0.8932 - val_loss: 0.1060 - val_accuracy: 0.8601\n",
      "Epoch 284/300\n",
      "37356/37356 - 1s - loss: 0.0819 - accuracy: 0.8940 - val_loss: 0.1059 - val_accuracy: 0.8627\n",
      "Epoch 285/300\n",
      "37356/37356 - 1s - loss: 0.0823 - accuracy: 0.8933 - val_loss: 0.1056 - val_accuracy: 0.8600\n",
      "Epoch 286/300\n",
      "37356/37356 - 1s - loss: 0.0823 - accuracy: 0.8931 - val_loss: 0.1057 - val_accuracy: 0.8585\n",
      "Epoch 287/300\n",
      "37356/37356 - 1s - loss: 0.0825 - accuracy: 0.8935 - val_loss: 0.1049 - val_accuracy: 0.8602\n",
      "Epoch 288/300\n",
      "37356/37356 - 1s - loss: 0.0826 - accuracy: 0.8921 - val_loss: 0.1068 - val_accuracy: 0.8575\n",
      "Epoch 289/300\n",
      "37356/37356 - 1s - loss: 0.0825 - accuracy: 0.8933 - val_loss: 0.1054 - val_accuracy: 0.8632\n",
      "Epoch 290/300\n",
      "37356/37356 - 1s - loss: 0.0826 - accuracy: 0.8932 - val_loss: 0.1104 - val_accuracy: 0.8534\n",
      "Epoch 291/300\n",
      "37356/37356 - 1s - loss: 0.0823 - accuracy: 0.8920 - val_loss: 0.1104 - val_accuracy: 0.8549\n",
      "Epoch 292/300\n",
      "37356/37356 - 1s - loss: 0.0823 - accuracy: 0.8942 - val_loss: 0.1074 - val_accuracy: 0.8610\n",
      "Epoch 293/300\n",
      "37356/37356 - 1s - loss: 0.0817 - accuracy: 0.8938 - val_loss: 0.1068 - val_accuracy: 0.8601\n",
      "Epoch 294/300\n",
      "37356/37356 - 1s - loss: 0.0815 - accuracy: 0.8954 - val_loss: 0.1092 - val_accuracy: 0.8540\n",
      "Epoch 295/300\n",
      "37356/37356 - 1s - loss: 0.0823 - accuracy: 0.8932 - val_loss: 0.1058 - val_accuracy: 0.8595\n",
      "Epoch 296/300\n",
      "37356/37356 - 1s - loss: 0.0818 - accuracy: 0.8937 - val_loss: 0.1053 - val_accuracy: 0.8604\n",
      "Epoch 297/300\n",
      "37356/37356 - 1s - loss: 0.0821 - accuracy: 0.8947 - val_loss: 0.1076 - val_accuracy: 0.8591\n",
      "Epoch 298/300\n",
      "37356/37356 - 1s - loss: 0.0813 - accuracy: 0.8963 - val_loss: 0.1084 - val_accuracy: 0.8591\n",
      "Epoch 299/300\n",
      "37356/37356 - 1s - loss: 0.0810 - accuracy: 0.8944 - val_loss: 0.1069 - val_accuracy: 0.8585\n",
      "Epoch 300/300\n",
      "37356/37356 - 1s - loss: 0.0809 - accuracy: 0.8955 - val_loss: 0.1062 - val_accuracy: 0.8618\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=300, batch_size=128, validation_data=(x_val, y_val), verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
